%---------- Inleiding ---------------------------------------------------------

% TODO: Is dit voorstel gebaseerd op een paper van Research Methods die je
% vorig jaar hebt ingediend? Heb je daarbij eventueel samengewerkt met een
% andere student?
% Zo ja, haal dan de tekst hieronder uit commentaar en pas aan.

%\paragraph{Opmerking}

% Dit voorstel is gebaseerd op het onderzoeksvoorstel dat werd geschreven in het
% kader van het vak Research Methods dat ik (vorig/dit) academiejaar heb
% uitgewerkt (met medesturent VOORNAAM NAAM als mede-auteur).
% 

\section{Inleiding}%
\label{sec:inleiding}

Het thema van deze bachelorproef is het vergelijken van verschillende self-hosted Large Language Models (LLMs) om hun geschiktheid voor het ondersteunen van codeertaken binnen bedrijfsomgevingen te bepalen. Met de toenemende afhankelijkheid van bedrijven van AI voor programmeerhulp, groeit ook de bezorgdheid over de veiligheid van gevoelige gegevens. Cloudgebaseerde modellen zoals ChatGPT of Gemini brengen het risico met zich mee dat bedrijfsgevoelige code wordt opgenomen in trainingssets, wat kan leiden tot datalekken. Voor bedrijven die data-privacy als prioriteit zien, bieden self-hosted LLMs een mogelijk veilig alternatief.  

De onderzoeksvraag luidt: 

\begin{quote} 
  "Welke self-hosted LLM biedt de beste ondersteuning voor codeertaken binnen bedrijven, terwijl de risico's op datalekken worden geminimaliseerd?" 
\end{quote}

Het doel van dit onderzoek is om bedrijven een objectieve evaluatie te bieden van de prestaties, nauwkeurigheid, efficiëntie en gebruiksvriendelijkheid van geselecteerde open-source LLMs, zoals StarCoder en Mistral. Hierbij worden modellen getest met behulp van gestandaardiseerde benchmarks, zoals HumanEval en MBPP, in een gecontroleerde self-hosted omgeving.  

Dit onderzoek beoogt bedrijven praktische aanbevelingen te geven over welke modellen het meest geschikt zijn voor veilige en efficiënte AI-integratie in hun ontwikkelprocessen, waarbij innovatie mogelijk blijft zonder de veiligheid van gevoelige gegevens in gevaar te brengen.

%---------- Stand van zaken ---------------------------------------------------

\section{Literatuurstudie}%
\label{sec:literatuurstudie}

De snelle ontwikkeling van Large Language Models (LLMs) heeft geleid tot brede toepassingen in domeinen zoals tekstgeneratie, vertaling en programmeerassistentie. Traditioneel worden deze modellen gehost in cloudomgevingen, zoals bij OpenAI's ChatGPT en Google's Gemini, wat aanzienlijke voordelen biedt op het gebied van schaalbaarheid en onderhoud. Echter, de vraag naar self-hosted alternatieven groeit snel, vooral in sectoren waar data-privacy en controle over gevoelige gegevens cruciaal zijn. Cloudgebaseerde oplossingen brengen namelijk het risico met zich mee dat gebruikersdata – zoals bedrijfsgevoelige code – wordt opgenomen in trainingsdatasets of anderszins wordt opgeslagen \autocite{Brown2020}.

Self-hosted LLMs bieden een oplossing voor dit probleem door bedrijven volledige controle te geven over de opslag en verwerking van data. Modellen zoals StarCoder, ontwikkeld door Hugging Face, en Mistral, een compact maar krachtige open-source model, zijn specifiek ontworpen om lokaal te worden ingezet en richten zich op efficiëntie en gebruiksvriendelijkheid \autocite{Tunstall2023, Team2023}. StarCoder is geoptimaliseerd voor codeertaken en ondersteunt meerdere programmeertalen, wat het geschikt maakt voor bredere toepassingen in softwareontwikkeling \autocite{Li2022}. Mistral daarentegen biedt een uitstekende balans tussen prestaties en hardwarevereisten, wat belangrijk is voor bedrijven die niet over uitgebreide infrastructuur beschikken \autocite{Mistral2023}.

Eerdere studies hebben benchmarks zoals HumanEval en MBPP gebruikt om de prestaties van LLMs op programmeergerelateerde taken te evalueren \autocite{Chen2021}. Deze benchmarks testen de nauwkeurigheid van codegeneratie en probleemoplossend vermogen op basis van gestandaardiseerde datasets. Bevindingen tonen aan dat modellen zoals OpenAI Codex indrukwekkende resultaten behalen in specifieke contexten, maar de afhankelijkheid van cloudhosting blijft een grote zorg voor bedrijven \autocite{Chen2021, Brown2020}. Dit heeft geleid tot een groeiende interesse in zelf-gehoste alternatieven, hoewel er nog weinig systematisch onderzoek is uitgevoerd naar hoe deze modellen presteren in een bedrijfscontext.

Een open vraag in het domein is hoe self-hosted LLMs zich verhouden tot cloudgebaseerde modellen, niet alleen op het gebied van prestaties, maar ook in termen van integratiegemak en resourcegebruik. Hoewel er case studies zijn die enkele aspecten van self-hosted modellen belichten, zoals hun hardwarevereisten \autocite{Rao2023}, blijft er een gebrek aan uitgebreide evaluaties die zich richten op de praktische toepasbaarheid binnen bedrijfsomgevingen.

Dit onderzoek vult deze leegte door een systematische evaluatie te bieden van self-hosted LLMs met een focus op gebruik in bedrijven. In tegenstelling tot eerdere studies, die zich voornamelijk richten op prestaties op academische benchmarks, zal deze studie ook aspecten zoals implementatiegemak, veiligheid en compatibiliteit met bestaande workflows beoordelen. Hierdoor wordt niet alleen inzicht gegeven in de technische mogelijkheden van self-hosted LLMs, maar wordt ook een praktische handleiding geboden voor bedrijven die overwegen deze technologie te adopteren.

%---------- Methodologie ------------------------------------------------------
\section{Methodologie}%
\label{sec:methodologie}

Het onderzoek zal worden uitgevoerd als een vergelijkende studie waarbij verschillende self-hosted Large Language Models (LLMs) systematisch worden geëvalueerd op basis van hun prestaties en geschiktheid voor gebruik in bedrijfsomgevingen. De gekozen onderzoekstechnieken zijn experimenten en literatuurstudie, gecombineerd met een Proof of Concept (PoC) om de praktische toepasbaarheid van de modellen te testen. Het doel is om objectieve, meetbare resultaten te verkrijgen die bedrijven kunnen helpen bij het selecteren van een model dat voldoet aan hun vereisten voor code-assistentie en datalekpreventie.

\subsection{Onderzoekstechnieken en stappen}

\begin{enumerate}
    \item \textbf{Literatuurstudie}
    \begin{itemize}
        \item Een grondige analyse van de huidige stand van zaken rond self-hosted LLMs en bestaande benchmarks voor codeertaken, zoals HumanEval en MBPP \autocite{Chen2021}.
        \item Het identificeren van geschikte modellen, zoals StarCoder en Mistral, en hun technische vereisten en sterke punten.
    \end{itemize}

    \item \textbf{Modelselectie en installatie}
    \begin{itemize}
        \item Selectie van 3-5 self-hosted LLMs op basis van literatuur en beschikbaarheid.
        \item Installatie van de modellen in een gecontroleerde testomgeving, gebruikmakend van hardware zoals GPU-clusters (bijvoorbeeld NVIDIA A100 of soortgelijke servers) en software-infrastructuur zoals Docker en Kubernetes voor modelhosting.
    \end{itemize}

    \item \textbf{Ontwikkeling van benchmarks en testcases}
    \begin{itemize}
        \item Samenstellen van een set programmeerproblemen in verschillende talen en moeilijkheidsgraden, gebaseerd op bestaande benchmarks zoals HumanEval.
        \item Ontwikkelen van testcases om specifieke prestatie-indicatoren te meten, zoals codegeneratienauwkeurigheid, responstijd en resourcegebruik.
    \end{itemize}

    \item \textbf{Experimenten uitvoeren}
    \begin{itemize}
        \item Testen van de modellen onder identieke omstandigheden.
        \item Verzamelen van meetbare data, zoals de gemiddelde tijd om een probleem op te lossen, de foutmarge in de gegenereerde code en het gebruik van systeembronnen.
        \item Veiligheidsanalyse van de modellen, inclusief een controle op data-uitwisseling en logging om te verifiëren dat gegevens niet extern worden opgeslagen.
    \end{itemize}

    \item \textbf{Proof of Concept (PoC)}
    \begin{itemize}
        \item Integratie van een geselecteerd model in een eenvoudige ontwikkelworkflow (bijvoorbeeld een lokaal geïnstalleerde IDE zoals Visual Studio Code met AI-plug-ins).
        \item Evaluatie van het gebruiksgemak en de impact op de ontwikkelsnelheid en -kwaliteit in een praktijkgerichte context.
    \end{itemize}
\end{enumerate}

\subsection{Tools en technologieën}

\begin{itemize}
    \item \textbf{Hardware:} NVIDIA GPU’s (bij voorkeur A100 of H100 voor optimale prestaties), lokale servers of een on-premises datacenter.
    \item \textbf{Software:} Docker, Kubernetes, Python, en tooling voor benchmarking zoals OpenAI’s evaluation scripts \autocite{Brown2020}.
    \item \textbf{Benchmarks:} HumanEval, MBPP, en eventueel aangepast voor specifieke bedrijfsomgevingen.
\end{itemize}

\subsection{Tijdschatting en deliverables}

\begin{itemize}
    \item \textbf{Literatuurstudie en modelselectie} (3 weken)  
    Deliverable: Overzicht van relevante LLMs en benchmarks, inclusief een korte vergelijking op basis van theoretische sterktes en vereisten.

    \item \textbf{Modelimplementatie en testomgeving opzetten} (4 weken)  
    Deliverable: Werkende self-hosted implementaties van geselecteerde modellen.

    \item \textbf{Ontwikkeling en validatie van benchmarks} (2 weken)  
    Deliverable: Een set gestandaardiseerde testcases en een geconfigureerde testomgeving.

    \item \textbf{Uitvoeren van experimenten} (4 weken)  
    Deliverable: Data en grafieken met prestatie-indicatoren en vergelijkingen.

    \item \textbf{Proof of Concept} (2 weken)  
    Deliverable: Werkende integratie van een LLM in een lokale ontwikkelworkflow, inclusief documentatie en bevindingen.

    \item \textbf{Analyse en rapportage} (3 weken)  
    Deliverable: Eindrapport met aanbevelingen en conclusie.
\end{itemize}

De totale geschatte duur van het onderzoek is 18 weken. Hiermee wordt voldaan aan de technische diepgang en de vereiste tijdsinvestering voor een bachelorproef in informatica.

%---------- Verwachte resultaten ----------------------------------------------
\section{Verwacht resultaat, conclusie}%
\label{sec:verwachte_resultaten}

\subsection{Verwacht resultaat}

Het onderzoek zal resulteren in een systematische evaluatie van self-hosted Large Language Models (LLMs) zoals StarCoder en Mistral op basis van hun prestaties en geschiktheid voor code-assistentie in bedrijfsomgevingen. De verwachte resultaten omvatten:

\begin{itemize}
    \item \textbf{Prestaties:} Een gedetailleerde vergelijking van de modellen op basis van metrics zoals:
    \begin{itemize}
        \item Codegeneratienauwkeurigheid (correctheid van de gegenereerde code).
        \item Responstijd (in seconden) voor verschillende programmeerproblemen.
        \item Resourcegebruik (GPU-geheugen, CPU-gebruik).
    \end{itemize}

    \item \textbf{Veiligheid:} Bevestiging dat self-hosted modellen geen data opslaan of externe verbindingen maken tijdens het uitvoeren van code-assistentie.

    \item \textbf{Praktische toepasbaarheid:} Een beoordeling van hoe eenvoudig deze modellen kunnen worden geïntegreerd in een typische ontwikkelworkflow, inclusief hun compatibiliteit met tools zoals Visual Studio Code of JetBrains IDE's.

    \item \textbf{Benchmarkresultaten:} Gevisualiseerde data zoals mock-up grafieken:
    \begin{itemize}
        \item Een staafdiagram dat de nauwkeurigheid per model weergeeft op verschillende programmeertaken.
        \item Een lijngrafiek die de responstijd vergelijkt bij het oplossen van problemen met oplopende complexiteit.
        \item Een spreidingsdiagram van het resourcegebruik tegenover de modelprestaties.
    \end{itemize}
\end{itemize}

\subsection{Conclusie}

De verwachting is dat StarCoder beter presteert op complexere programmeerproblemen vanwege de focus op codeertaken in de training, terwijl Mistral meer geschikt blijkt voor resource-efficiënte omgevingen door zijn kleinere modelgrootte en lagere hardwarevereisten. Beide modellen worden als geschikte kandidaten gezien voor specifieke bedrijfscontexten, afhankelijk van de prioriteit op nauwkeurigheid of kosten.

De meerwaarde van dit onderzoek ligt in het bieden van een concrete handleiding aan bedrijven die op zoek zijn naar veilige en efficiënte AI-oplossingen voor programmeerhulp. Dit helpt organisaties bij het maken van een weloverwogen keuze en ondersteunt de adoptie van self-hosted LLMs zonder datalekrisico’s. Hierdoor kunnen bedrijven innovatie en dataveiligheid combineren in hun ontwikkelprocessen. Eventuele afwijkingen van de hypothese, zoals onverwacht betere prestaties van een model of integratieproblemen, zullen nader worden onderzocht om een dieper inzicht te krijgen in de praktische toepasbaarheid van deze technologie.