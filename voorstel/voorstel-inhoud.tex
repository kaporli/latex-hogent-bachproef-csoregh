%---------- Inleiding ---------------------------------------------------------

% TODO: Is dit voorstel gebaseerd op een paper van Research Methods die je
% vorig jaar hebt ingediend? Heb je daarbij eventueel samengewerkt met een
% andere student?
% Zo ja, haal dan de tekst hieronder uit commentaar en pas aan.

%\paragraph{Opmerking}

% Dit voorstel is gebaseerd op het onderzoeksvoorstel dat werd geschreven in het
% kader van het vak Research Methods dat ik (vorig/dit) academiejaar heb
% uitgewerkt (met medesturent VOORNAAM NAAM als mede-auteur).
% 

\section{Inleiding}%
\label{sec:inleiding}

Het thema van deze bachelorproef is het vergelijken van verschillende self-hosted Large Language Models (LLMs) om hun geschiktheid voor het ondersteunen van codeertaken binnen bedrijfsomgevingen te bepalen. Met de toenemende afhankelijkheid van bedrijven van AI voor programmeerhulp groeit ook de bezorgdheid over de veiligheid van gevoelige gegevens. Cloudgebaseerde modellen zoals ChatGPT of Gemini brengen het risico met zich mee dat bedrijfsgevoelige code wordt opgenomen in trainingssets, wat kan leiden tot datalekken. Voor bedrijven die data-privacy als prioriteit zien, bieden self-hosted LLMs een mogelijk veilig alternatief.

De onderzoeksvraag luidt:

"Welke self-hosted LLM biedt de beste ondersteuning voor codeertaken binnen bedrijven, terwijl de risico’s op datalekken worden geminimaliseerd?"

Om deze vraag te beantwoorden, worden de volgende deelvragen geformuleerd:

\begin{enumerate}
    \item Welke benchmarks zijn het meest geschikt om de prestaties van self-hosted LLMs te evalueren?

    \item Wat zijn de hardware- en softwarevereisten voor de implementatie van self-hosted LLMs?

    \item Hoe presteren de geselecteerde modellen op het gebied van nauwkeurigheid, responstijd en resourcegebruik?

    \item Hoe eenvoudig zijn self-hosted LLMs te integreren in bestaande ontwikkelworkflows?

\end{enumerate}

Het doel van dit onderzoek is om bedrijven een objectieve evaluatie te bieden van de prestaties, nauwkeurigheid, efficiëntie en gebruiksvriendelijkheid van geselecteerde open-source LLMs, zoals StarCoder en Mistral. Hierbij worden modellen getest met behulp van gestandaardiseerde benchmarks, zoals HumanEval en MBPP, in een gecontroleerde self-hosted omgeving.

%---------- Stand van zaken ---------------------------------------------------

\section{Literatuurstudie}%
\label{sec:literatuurstudie}

De snelle ontwikkeling van Large Language Models (LLMs) heeft geleid tot brede toepassingen in uiteenlopende domeinen, variërend van tekstgeneratie en vertaling tot programmeerassistentie. Traditioneel zijn deze modellen in cloudomgevingen gehost, zoals bij OpenAI's ChatGPT en het binnenkort beschikbare Gemini van Google. Deze cloudgebaseerde oplossingen bieden aanzienlijke voordelen op het gebied van schaalbaarheid, onderhoud en directe toegankelijkheid \autocite{Brown2020}. Toch groeit de vraag naar self-hosted alternatieven snel, vooral binnen bedrijven die waarde hechten aan dataprivacy en volledige controle over hun infrastructuur. Een belangrijk aandachtspunt hierbij is het risico dat bedrijfsgevoelige data – inclusief broncode en intellectueel eigendom – onbedoeld terechtkomt in externe trainingsdatasets of op systemen buiten de eigen controle \autocite{Rao2023}.

Self-hosted LLMs proberen deze uitdagingen aan te pakken door bedrijven in staat te stellen de modellen lokaal, of op hun eigen servers, te draaien. Hierdoor behouden organisaties de regie over de opslag en verwerking van hun data en minimaliseren zij het risico op datalekken. Modellen zoals StarCoder, ontwikkeld door Hugging Face, en Mistral, een compact maar krachtig open-source model, zijn specifiek ontworpen met deze doelstellingen voor ogen. StarCoder richt zich met name op codeertaken en ondersteunt meerdere programmeertalen, waardoor het een brede inzetbaarheid kent in softwareontwikkelingsteams \autocite{Li2022, Tunstall2023}. Mistral is daarentegen geoptimaliseerd voor efficiëntie en lagere hardwarevereisten, wat het aantrekkelijk maakt voor bedrijven met beperktere infrastructuur \autocite{Mistral2023, Team2023}.

Eerdere studies hebben al laten zien dat benchmarks zoals HumanEval en MBPP waardevol zijn om de codegeneratieprestaties van LLMs te meten \autocite{Chen2021}. Deze benchmarks testen niet alleen de nauwkeurigheid van de gegenereerde code, maar ook het probleemoplossend vermogen van de modellen in gestandaardiseerde scenario’s. Tot nu toe lag de focus daarbij voornamelijk op cloudgebaseerde oplossingen, zoals OpenAI Codex, die indrukwekkende resultaten hebben laten zien. Echter, het gebrek aan vergelijkend onderzoek naar self-hosted alternatieven en hun toepasbaarheid in een bedrijfsomgeving blijft een belangrijke leemte in de literatuur.

Deze leemte wordt nog duidelijker wanneer men de praktische aspecten van de implementatie van self-hosted modellen in beschouwing neemt. Hoewel er case studies zijn die bepaalde technische aspecten en resourcevereisten van self-hosted LLMs belichten \autocite{Rao2023}, blijft er een gebrek aan systematische evaluaties die niet alleen prestaties, maar ook integratiegemak, veiligheid en compatibiliteit met bestaande ontwikkelomgevingen onder de loep nemen. Dit zijn juist de factoren die in een bedrijfscontext essentieel zijn voor de adoptie en het succes van dergelijke technologie.

Dit onderzoek bouwt voort op bestaande literatuur, maar gaat verder door ook de volgende concrete vragen te adresseren: welke benchmarks zijn het meest geschikt om de prestaties van self-hosted LLMs te evalueren, welke hardware- en softwarevereisten zijn noodzakelijk, hoe presteren de gekozen modellen op het gebied van nauwkeurigheid, responstijd en resourcegebruik, en in hoeverre kunnen ze eenvoudig worden geïntegreerd in bestaande ontwikkelworkflows? Door deze aspecten samen te brengen, zal deze studie niet alleen inzicht bieden in de technische prestaties van modellen als StarCoder en Mistral, maar ook een praktische richtlijn vormen voor bedrijven die de overstap naar self-hosted LLMs overwegen. Daarmee draagt dit onderzoek bij aan een meer holistische benadering van LLM-evaluatie, waarin zowel technische als organisatorische factoren worden meegenomen.
%---------- Methodologie ------------------------------------------------------
\section{Methodologie}%
\label{sec:methodologie}

Dit onderzoek volgt een systematische aanpak om de onderzoeksvraag te beantwoorden. De onderzoeksmethoden zijn ingericht rond de eerder geformuleerde deelvragen:

\begin{enumerate}
    \item Welke benchmarks zijn het meest geschikt om de prestaties van self-hosted LLMs te evalueren?

    \item Wat zijn de hardware- en softwarevereisten voor de implementatie van self-hosted LLMs?

    \item Hoe presteren de geselecteerde modellen op het gebied van nauwkeurigheid, responstijd en resourcegebruik?

    \item Hoe eenvoudig zijn self-hosted LLMs te integreren in bestaande ontwikkelworkflows?

\end{enumerate}

Door het combineren van literatuurstudie, experimentele evaluaties en een Proof of Concept (PoC), wordt een geïntegreerd beeld verkregen. Onderstaande stappen en technieken zijn specifiek gekoppeld aan de deelvragen.

\subsection{Onderzoekstechnieken en stappen}

\begin{enumerate}
    \item \textbf{Literatuurstudie}
    \begin{itemize}
        \item In deze fase worden bestaande studies, documentatie en vakliteratuur onderzocht om inzicht te krijgen in welke benchmarks momenteel worden gebruikt voor het evalueren van codegerichte LLMs. Daarnaast worden de hardware- en softwarevereisten in kaart gebracht aan de hand van documentatie van modelontwikkelaars en ervaringen uit de praktijk. Dit leidt tot:
        \begin{itemize}
            \item Identificatie van relevante benchmarks zoals HumanEval en MBPP \autocite{Chen2021}.
            \item Het opstellen van een overzicht van noodzakelijke infrastructuur (bijvoorbeeld NVIDIA GPU’s, Docker, Kubernetes) voor het zelf hosten van LLMs.
        \end{itemize}
    \end{itemize}

    \item \textbf{Modelselectie en installatie}
    \begin{itemize}
        \item Op basis van de literatuurstudie worden 3-5 self-hosted LLMs geselecteerd (bijvoorbeeld StarCoder, Mistral). Deze modellen worden geïnstalleerd in een gecontroleerde testomgeving. Hierbij wordt gelet op:
        \begin{itemize}
            \item De vereisten voor hardware (GPU’s) en software (model-implementaties, container- en orkestratietools).
            \item Het controleren van de haalbaarheid en complexiteit van de set-up, wat inzichten biedt voor deelvraag 2.
        \end{itemize}
    \end{itemize}

    \item \textbf{Ontwikkeling van benchmarks en testcases}
    \begin{itemize}
        \item Op basis van de literatuurstudie worden benchmarks en testcases samengesteld om de prestaties van de geselecteerde modellen te meten. Hierbij gaat het om:
        \begin{itemize}
            \item Het selecteren of aanpassen van bestaande benchmarks (HumanEval, MBPP) voor codegeneratietaken, zodat zij representatief zijn voor bedrijfsrelevante scenario’s.
            \item Het opstellen van prestatie-indicatoren als nauwkeurigheid (correctheid van gegenereerde code), responstijd (tijd tot bruikbaar antwoord), en resourcegebruik (GPU-gebruik, geheugengebruik).
            Dit beantwoordt deels deelvraag 1 (keuze van benchmarks) en vormt de meetlat voor deelvraag 3 (prestatie-indicatoren).
        \end{itemize}
    \end{itemize}

    \item \textbf{Experimenten uitvoeren}
    \begin{itemize}
        \item Alle geselecteerde modellen worden onder identieke omstandigheden getest. Hierbij wordt:
        \begin{itemize}
            \item De nauwkeurigheid bepaald aan de hand van gestandaardiseerde testcases.
            \item De responstijd gemeten op identieke codeerproblemen.
            \item Het resourcegebruik (GPU, CPU, geheugen) bijgehouden.
            \item Ook worden veiligheidscontroles uitgevoerd (bijv. controle op datalekken of externe dataopslag) om de geschiktheid voor een bedrijfsomgeving te beoordelen.
            De uitkomsten van deze experimenten geven een duidelijk antwoord op deelvraag 3.
            Dit beantwoordt deels deelvraag 1 (keuze van benchmarks) en vormt de meetlat voor deelvraag 3 (prestatie-indicatoren).
        \end{itemize}
    \end{itemize}

    \item \textbf{Proof of Concept (PoC)}
    \begin{itemize}
        \item Om te onderzoeken hoe eenvoudig de modellen kunnen worden geïntegreerd in bestaande ontwikkelomgevingen, wordt een PoC opgezet. Hierbij wordt een geselecteerd model geïntegreerd in een ontwikkelworkflow:
        \begin{itemize}
            \item Inzet van een LLM in een lokaal geïnstalleerde IDE (bijvoorbeeld Visual Studio Code) met AI-plug-ins.
            \item Evaluatie van de gebruiksvriendelijkheid, de benodigde aanpassingen aan de ontwikkelworkflow en de daadwerkelijke tijdswinst en codekwaliteit.
            Dit proces levert direct inzicht in de praktische toepasbaarheid en beantwoordt deelvraag 4.
        \end{itemize}
    \end{itemize}
\end{enumerate}

\subsection{Tools en technologieën}

\begin{itemize}
    \item \textbf{Hardware:} Hardware: NVIDIA GPU’s (A100/H100) voor optimale prestatie.
    \item \textbf{Software:} Software: Docker, Kubernetes voor modelhosting en schaalbaarheid; Python voor scripts en analyses; benchmarking tools (bijvoorbeeld OpenAI’s evaluation scripts \autocite{Brown2020}).
    \item \textbf{Benchmarks:} Benchmarks: HumanEval, MBPP, mogelijk aangevuld met bedrijfsrelevante testcases.
\end{itemize}

\subsection{Tijdschatting en deliverables}

\begin{itemize}
    \item \textbf{Literatuurstudie en modelselectie} (3 weken)  
    Deliverable: Overzicht relevante benchmarks, infrastructuurvereisten, modelkeuze.

    \item \textbf{Modelimplementatie en testomgeving opzetten} (4 weken)  
    Deliverable: Werkende self-hosted implementaties van geselecteerde modellen.

    \item \textbf{Ontwikkeling en validatie van benchmarks} (2 weken)  
    Deliverable: Een set gestandaardiseerde testcases en een geconfigureerde testomgeving.

    \item \textbf{Uitvoeren van experimenten} (4 weken)  
    Deliverable: Data en grafieken met prestatie-indicatoren en vergelijkingen.

    \item \textbf{Proof of Concept} (2 weken)  
    Deliverable: Werkende integratie van een LLM in een lokale ontwikkelworkflow, inclusief documentatie en bevindingen.

    \item \textbf{Analyse en rapportage} (3 weken)  
    Deliverable: Eindrapport met conclusies, aanbevelingen en beantwoording van alle deelvragen.
\end{itemize}

Door deze aanpak worden alle deelvragen direct of indirect beantwoord en ontstaat een volledig beeld van de praktische inzetbaarheid van self-hosted LLMs in een bedrijfscontext.

%---------- Verwachte resultaten ----------------------------------------------
\section{Verwacht resultaat, conclusie}%
\label{sec:verwachte_resultaten}

\subsection{Verwacht resultaat}

Het onderzoek zal resulteren in een systematische evaluatie van self-hosted Large Language Models (LLMs) zoals StarCoder en Mistral op basis van hun prestaties en geschiktheid voor code-assistentie in bedrijfsomgevingen. De verwachte resultaten worden hierbij gerelateerd aan de gestelde deelvragen:
\begin{itemize}
    \item \textbf{(deelvraag 1)} Benchmarks
    \begin{itemize}
        \item Een duidelijk overzicht van welke benchmarks (bijvoorbeeld HumanEval, MBPP) het meest geschikt zijn voor het evalueren van self-hosted LLMs. Dit omvat een onderbouwing waarom deze benchmarks representatief zijn voor reële programmeerproblemen binnen bedrijven.
    \end{itemize}

    \item \textbf{(deelvraag 2)} Implementatievereisten
    \begin{itemize}
        \item Een specificatie van de benodigde hardware- en softwarevereisten voor het zelf hosten van LLMs, inclusief aanbevelingen voor GPU-configuraties, container- en orkestratietools, en richtlijnen voor het optimaal inzetten van modelresources.
    \end{itemize}

    \item \textbf{Praktische toepasbaarheid:} Een beoordeling van hoe eenvoudig deze modellen kunnen worden geïntegreerd in een typische ontwikkelworkflow, inclusief hun compatibiliteit met tools zoals Visual Studio Code of JetBrains IDE's.

    \item \textbf{(deelvraag 3)} Prestaties
    \item Een gedetailleerde vergelijking van de geselecteerde modellen met betrekking tot:
    \begin{itemize}
        \item Codegeneratienauwkeurigheid: Mate waarin de gegenereerde code correct en functioneel is, gemeten aan de hand van vooraf gedefinieerde testcases.
        \item Responstijd: De gemiddelde tijd (in seconden) om een programmeerprobleem op te lossen, gemeten voor taken van verschillende complexiteitsniveaus.
        \item Resourcegebruik: Het GPU-geheugen- en CPU-verbruik tijdens het genereren van code, vastgelegd in kwantitatieve metingen.
    \end{itemize}
    \item Deze resultaten worden ondersteund door gevisualiseerde data, zoals:
    \begin{itemize}
        \item Een staafdiagram voor nauwkeurigheid per model per taak.
        \item Een lijngrafiek voor responstijd in relatie tot taakcomplexiteit.
        \item Een spreidingsdiagram voor resourcegebruik tegenover modelprestaties.
    \end{itemize}
    \item Daarnaast wordt er aandacht besteed aan veiligheid: bevestiging dat de zelfgehoste modellen geen data opslaan of externe verbindingen leggen, wat cruciaal is voor het minimaliseren van datalekrisico’s.
    
    \item \textbf{(deelvraag 4)} Integratiegemak
    \item Een beoordeling van hoe eenvoudig deze modellen kunnen worden geïntegreerd in een typische ontwikkelworkflow. Hierbij wordt beschreven:
    \begin{itemize}
        \item De compatibiliteit met ontwikkelomgevingen (zoals Visual Studio Code of JetBrains IDE’s).
        \item De benodigde configuratiestappen, tools en plug-ins.
        \item De invloed op ontwikkelsnelheid en codekwaliteit vanuit het perspectief van een eindgebruiker.
    \item Deze resultaten bieden organisaties een gefundeerd overzicht van de voor- en nadelen van self-hosted LLMs, en dienen als leidraad bij de selectie en integratie van dergelijke modellen in hun eigen ontwikkelpraktijk.
    \end{itemize}
\end{itemize}

\subsection{Conclusie}

De verwachting is dat StarCoder beter presteert op complexere programmeerproblemen vanwege de focus op codeertaken in de training, terwijl Mistral meer geschikt blijkt voor resource-efficiënte omgevingen door zijn kleinere modelgrootte en lagere hardwarevereisten. Beide modellen worden als geschikte kandidaten gezien voor specifieke bedrijfscontexten, afhankelijk van de prioriteit op nauwkeurigheid of kosten.

De meerwaarde van dit onderzoek ligt in het bieden van een concrete handleiding aan bedrijven die op zoek zijn naar veilige en efficiënte AI-oplossingen voor programmeerhulp. Dit helpt organisaties bij het maken van een weloverwogen keuze en ondersteunt de adoptie van self-hosted LLMs zonder datalekrisico’s. Hierdoor kunnen bedrijven innovatie en dataveiligheid combineren in hun ontwikkelprocessen. Eventuele afwijkingen van de hypothese, zoals onverwacht betere prestaties van een model of integratieproblemen, zullen nader worden onderzocht om een dieper inzicht te krijgen in de praktische toepasbaarheid van deze technologie.