%%=============================================================================
%% Methodologie
%%=============================================================================
\chapter{\IfLanguageName{dutch}{Methodologie}{Methodology}}%
\label{ch:methodologie}

%% TODO: In dit hoofstuk geef je een korte toelichting over hoe je te werk bent
%% gegaan. Verdeel je onderzoek in grote fasen, en licht in elke fase toe wat
%% de doelstelling was, welke deliverables daar uit gekomen zijn, en welke
%% onderzoeksmethoden je daarbij toegepast hebt. Verantwoord waarom je
%% op deze manier te werk gegaan bent.
%% 
%% Voorbeelden van zulke fasen zijn: literatuurstudie, opstellen van een
%% requirements-analyse, opstellen long-list (bij vergelijkende studie),
%% selectie van geschikte tools (bij vergelijkende studie, "short-list"),
%% opzetten testopstelling/PoC, uitvoeren testen en verzamelen
%% van resultaten, analyse van resultaten, ...
%%
%% !!!!! LET OP !!!!!
%%
%% Het is uitdrukkelijk NIET de bedoeling dat je het grootste deel van de corpus
%% van je bachelorproef in dit hoofstuk verwerkt! Dit hoofdstuk is eerder een
%% kort overzicht van je plan van aanpak.
%%
%% Maak voor elke fase (behalve het literatuuronderzoek) een NIEUW HOOFDSTUK aan
%% en geef het een gepaste titel.

This chapter provides an overview of the methodology applied in this thesis, describing the systematic approach used to answer the research question and achieve the stated objectives. The methodology is structured into three distinct phases, each with clear goals, deliverables, and research techniques, ensuring a robust evaluation of self-hosted Large Language Models (LLMs) for coding assistance.
\section{Research Phases}
\label{sec:research-phases}

\subsection{Phase 1: Literature Study, Model Selection, and Deployment}

The first phase begins with a comprehensive literature study investigating the evolution of AI-based code assistance and secure coding practices. This literature review forms the theoretical foundation for model selection, tracing the progression from traditional rule-based tools toward transformer-based LLMs and highlighting significant developments in code assistance, security implications, and enterprise adoption.
Guided by insights from this literature review, several self-hosted LLMs suitable for secure coding assistance are selected based on clearly defined criteria, including model architecture, parameter size, training corpus quality, availability of pre-trained code-specific weights, and demonstrated performance on coding tasks.
To ensure rigorous evaluation, standardized benchmarks widely recognized in previous studies are utilized. These benchmarks offer diversity in problem complexity, programming domains, and execution requirements, enabling a comprehensive and reproducible assessment.
The selected models are deployed using Ollama in an isolated offline computing environment. Ollama is specifically chosen for its compatibility with various open-source LLMs, ease of local deployment, and efficient resource management, ensuring fairness and consistency in testing conditions.

\paragraph{Deliverables of Phase 1:}
\begin{itemize}
	\item A comprehensive literature review on AI-based coding assistance, security considerations, and enterprise adoption.
	\item A curated selection of locally hosted models.
	\item A curated selection of standardized benchmarks for reproducible evaluations.
\end{itemize}

\subsection{Phase 2: Benchmarking and Performance Evaluation}

In the second phase, the selected models undergo structured benchmarking using the standardized evaluation tasks identified in Phase 1. Evaluation metrics include functional correctness (e.g., pass@k accuracy), reasoning capability, and efficiency. The solutions generated by the models are automatically validated against predefined test cases to ensure consistency and objectivity.
Evaluations are conducted under uniform resource allocations within a controlled and secure environment. Execution of generated code is sandboxed to prevent unintended interactions, ensuring reliability and reproducibility of results.

\paragraph{Deliverables of Phase 2:}

\begin{itemize}
	\item Detailed overview of benchmarking results.
\end{itemize}

\subsection{Phase 3: Results Analysis and Practical Integration}

The third phase involves analyzing benchmarking results to assess the practical feasibility of integrating self-hosted LLMs into enterprise software development workflows. Practical implications are examined, particularly focusing on how effectively these models support software engineering tasks such as code generation, refactoring, and debugging.
A comparative analysis with leading cloud-based LLMs (e.g., ChatGPT, Gemini, Claude) is conducted, considering key factors such as deployment costs, maintainability, ease of integration, and performance trade-offs.
Additionally, this phase identifies potential optimization strategies, such as fine-tuning and other inference acceleration methods, to further enhance model performance and usability in enterprise contexts.

\paragraph{Deliverables of Phase 3:}

\begin{itemize}
	\item Comprehensive comparative analysis of model performance across benchmarking tasks.
	\item Identification and discussion of key challenges and practical limitations associated with self-hosted AI code assistants.
\end{itemize}

\section{Justification of Approach}
\label{sec:justification-of-approach}

The selected research methodology integrates theoretical insights and empirical validation within a structured, phased approach. The literature study establishes foundational knowledge guiding model and benchmark selection. The standardized benchmarking ensures reliable empirical evidence, while the final analysis contextualizes the findings within practical enterprise scenarios.
By combining theoretical foundations with rigorous empirical evaluation, this research provides robust and actionable insights into deploying self-hosted LLMs securely and effectively in enterprise coding environments.

\section{Expected Results and Impact}
\label{sec:expected-results-and-impact}

\textbf{Expected Results:} The methodology is expected to yield validated benchmarks, insights into functional correctness, reasoning capability, and computational efficiency of self-hosted LLMs, and provide clarity on their practical applicability in software engineering workflows.

\paragraph{Impact:}


\begin{itemize}
	\item \textbf{Academic Contribution:} Advances empirical knowledge and methodological clarity regarding LLM performance, providing benchmarks and evaluation frameworks for future research.
	\item \textbf{Practical Contribution:} Delivers actionable insights enabling enterprises to make informed decisions regarding the adoption of self-hosted LLMs, balancing security, performance, and practical integration considerations.
\end{itemize}