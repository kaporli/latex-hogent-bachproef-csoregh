%%=============================================================================
%% Methodologie
%%=============================================================================
\chapter{\IfLanguageName{dutch}{Methodologie}{Methodology}}%
\label{ch:methodologie}

%% TODO: In dit hoofstuk geef je een korte toelichting over hoe je te werk bent
%% gegaan. Verdeel je onderzoek in grote fasen, en licht in elke fase toe wat
%% de doelstelling was, welke deliverables daar uit gekomen zijn, en welke
%% onderzoeksmethoden je daarbij toegepast hebt. Verantwoord waarom je
%% op deze manier te werk gegaan bent.
%% 
%% Voorbeelden van zulke fasen zijn: literatuurstudie, opstellen van een
%% requirements-analyse, opstellen long-list (bij vergelijkende studie),
%% selectie van geschikte tools (bij vergelijkende studie, "short-list"),
%% opzetten testopstelling/PoC, uitvoeren testen en verzamelen
%% van resultaten, analyse van resultaten, ...
%%
%% !!!!! LET OP !!!!!
%%
%% Het is uitdrukkelijk NIET de bedoeling dat je het grootste deel van de corpus
%% van je bachelorproef in dit hoofstuk verwerkt! Dit hoofdstuk is eerder een
%% kort overzicht van je plan van aanpak.
%%
%% Maak voor elke fase (behalve het literatuuronderzoek) een NIEUW HOOFDSTUK aan
%% en geef het een gepaste titel.

This chapter provides an overview of the methodology applied in this thesis, describing the systematic approach used to answer the research question and achieve the stated objectives. The methodology is structured into three distinct phases, each with clear goals, deliverables, and research techniques, ensuring a robust evaluation of self-hosted \glspl{LLM} for coding assistance.
\section{Research Phases}
\label{sec:research-phases}

\subsection{Phase 1: Literature Study, Model Selection, and Deployment Strategy}

The first phase begins with a structured literature study, presented in Chapter~\ref{ch:stand-van-zaken}, intended to inform the subsequent selection of models, definition of deployment constraints, and identification of suitable benchmarks. This study serves as the theoretical basis for understanding the evolution of \glsentryshort{AI}-based code assistance, considerations around secure software development, and factors relevant to enterprise adoption.

Following the literature study, criteria for model selection and deployment will be established, taking into account technical feasibility, security, and usability within enterprise contexts. Similarly, appropriate benchmarks will be selected to support reproducible and meaningful evaluation across a range of coding tasks.

The planned model selection approach, deployment strategy, and benchmark design are documented in Chapter~\ref{ch:proof-of-concept}, which outlines the framework used to prepare the evaluation setup.

\paragraph{Deliverables of Phase 1:}
\begin{itemize}
	\item A comprehensive literature study on \glsentryshort{AI}-based coding assistance, security considerations, and enterprise adoption.
	\item A curated selection of locally hosted models.
	\item A curated selection of standardized benchmarks for reproducible evaluations.
\end{itemize}

\subsection{Phase 2: Standardized Evaluation of Code Generation Performance}

In the second phase, the selected models will be evaluated through standardized benchmarks designed to test code generation performance. Primary focus will be placed on functional correctness, measured through pass@1 accuracy. Additional attention will be given to prompt stability and generalizability. The solutions generated by the models are automatically validated against predefined test cases to ensure consistency and objectivity.

Evaluations are conducted under uniform resource allocations within a controlled and secure environment, as detailed in Chapter~\ref{ch:results}. Execution of generated code is sandboxed to prevent unintended interactions, ensuring reliability and reproducibility of results.

\paragraph{Deliverables of Phase 2:}

\begin{itemize}
	\item Detailed overview of benchmarking results.
\end{itemize}

\subsection{Phase 3: Enterprise Integration and Comparative Results Analysis}

In the third phase, the benchmarking results will be analyzed to evaluate the practical viability of integrating self-hosted \glspl{LLM} into enterprise software development workflows. This includes assessing how well the models support core development tasks such as code generation and basic editing. A comparative discussion will be undertaken, referencing both proprietary cloud-based systems (e.g., ChatGPT, Gemini) and larger open-weight models that exceed local deployment constraints, to contextualize the capabilities of the tested models. This phase will reflect on trade-offs in performance, scalability, and operational fit from an enterprise perspective, and directly informs the conclusions drawn in Chapter~\ref{ch:conclusie}.

\paragraph{Deliverables of Phase 3:}

\begin{itemize}
	\item Comprehensive comparative analysis of model performance across benchmarking tasks.
	\item Identification and discussion of key challenges and practical limitations associated with self-hosted \glsentryshort{AI} code assistants.
\end{itemize}

\section{Justification of Approach}
\label{sec:justification-of-approach}

The selected research methodology integrates theoretical insights and empirical validation within a structured, phased approach. The literature study establishes foundational knowledge guiding model and benchmark selection. The standardized benchmarking ensures reliable empirical evidence, while the final analysis contextualizes the findings within practical enterprise scenarios.
By combining theoretical foundations with rigorous empirical evaluation, this research provides robust and actionable insights into deploying self-hosted \glspl{LLM} securely and effectively in enterprise coding environments.

\section{Expected Results and Impact}
\label{sec:expected-results-and-impact}

\textbf{Expected Results:} The methodology is expected to yield reproducible benchmark data and insights into the functional reliability and limitations of self-hosted \glspl{LLM} for code generation. It will also help clarify their potential role in real-world development workflows, particularly under constraints relevant to secure enterprise environments.

\paragraph{Impact:}

\begin{itemize}
	\item \textbf{Academic Contribution:} Advances empirical knowledge and methodological clarity regarding \gls{LLM} performance, providing benchmarks and evaluation frameworks for future research.
	\item \textbf{Practical Contribution:} Delivers actionable insights enabling enterprises to make informed decisions regarding the adoption of self-hosted \glspl{LLM}, balancing security, performance, and practical integration considerations.
\end{itemize}