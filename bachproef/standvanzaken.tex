\chapter{\IfLanguageName{dutch}{Literatuurstudie}{Literature Study}}%
\label{ch:stand-van-zaken}

% Tip: Begin elk hoofdstuk met een paragraaf inleiding die beschrijft hoe
% dit hoofdstuk past binnen het geheel van de bachelorproef. Geef in het
% bijzonder aan wat de link is met het vorige en volgende hoofdstuk.

% Pas na deze inleidende paragraaf komt de eerste sectiehoofding.

\section{Historical Evolution of Developer Tools (Pre-AI Era)}
\label{sec:historical-developer-tools}
Before the advent of modern \gls{AI} code assistants, software developers relied on an evolving ecosystem of deterministic tools designed to optimize coding efficiency, correctness, and maintainability. Early \glspl{IDE} introduced features such as syntax highlighting, static type checking, and rule-based autocompletion \autocite{Murphy2006IDEUsage}. Another important category of pre-\gls{AI} developer support tools was static analysis and linting. Static analyzers and linters, including \texttt{lint}, \texttt{Pylint}, and \texttt{ESLint}, further enhanced software quality by identifying style violations, runtime risks, and basic security vulnerabilities without executing code. These utilities accelerated development but operated entirely through manually defined heuristics and static language rules rather than adaptive learning \autocite{Ayewah2008}. The scope of such tools remained entirely diagnostic: they flagged issues but did not attempt to rewrite, transform, or generate code to fix them. This contrast between \emph{non-transformative} tools, which merely flag issues or offer basic, rule-based completions, and \emph{transformative} systems, which can actively generate, rewrite, or adapt code in real time, illustrates the fundamental shift introduced by \glspl{LLM} \autocite{xu2022systematic}. \emph{Non-transformative}  tools are reactive and constrained by predefined heuristics; their suggestions reflect only what developers have manually encoded. \emph{Transformative} \gls{AI} assistants, on the other hand, engage more dynamically with the codebase, extending beyond static rules to contribute meaningfully to the development process.

One of the most dynamic and enduring sources of support has been community-driven platforms like Stack Overflow, which continue to provide curated solutions, though they require manual search and interpretation. Comparative studies show that despite the rise of \gls{AI} chatbots like ChatGPT, many developers still turn to Stack Overflow for trusted, community-vetted answers, especially when depth or discussion is needed \autocite{Liu2023Comparative}.

Some early academic work foreshadowed the shift from static rule-based tools to data-driven developer assistance. Hindle et al.\ demonstrated that source code, like natural language, exhibits strong statistical patterns: developers tend to write predictable sequences of tokens that can be modeled using n-gram language models, which estimate the likelihood of a token based on its preceding context \autocite{Hindle2012}. These findings suggested that code completion could be enhanced using probabilistic methods rather than fixed rules. Subsequent work expanded on this idea, with researchers like Allamanis et al.\ exploring statistical models for tasks such as API suggestion and bug detection \autocite{Allamanis2018}. Another strand of research used similar techniques to generate natural-language comments from source code, further highlighting the parallel between code and language modeling \autocite{MovshovitzAttias2013}.

However, these early approaches were limited to surface-level patterns. They lacked semantic understanding, operated at the token level, and were incapable of generating coherent code from scratch. Before the rise of deep learning, learned models could only augment traditional tools in narrow ways, offering token predictions or comment suggestions, but not synthesizing new logic or adapting to a developer’s broader intent.

\section{Emergence of AI-Powered Code Assistance}
\label{sec:emergence-ai-code-assistance}

Building upon the earlier use of deterministic and statistical developer tools described in Section~\ref{sec:historical-developer-tools}, the late 2010s marked a decisive shift toward learned representations and deep learning. This transition was catalyzed by the introduction of Transformer architectures, which utilize self-attention mechanisms to capture long-range dependencies within sequences. Originally developed for \gls{NLP} tasks, Transformers have proven effective in modeling the syntactic and semantic structures of source code, which, like natural language, exhibits sequential and hierarchical patterns. Researchers began adapting these architectures to code-related tasks, such as code completion, function naming, and bug fixing, demonstrating that Transformers could learn meaningful representations of code syntax and semantics at scale . This advancement enabled models to surpass the limitations of traditional n-gram models and rule-based systems, facilitating more sophisticated and context-aware code generation and analysis \autocite{Chirkova2021}.

This development gave rise to transformer-based models such as CodeBERT~\autocite{Feng2020} and CodeT5~\autocite{wang2021codet5}, which were pretrained on large corpora of code and natural language, and then fine-tuned for downstream tasks like summarization, defect detection, and code generation. These models demonstrated that transformer architectures could be successfully adapted to the structural complexities of code, bridging natural language understanding and code synthesis within a shared representational framework.

By 2019, prototype autoregressive models for real-time code completion began entering developer environments. TabNine, for instance, incorporated a GPT-2 model into an \gls{IDE} plugin, demonstrating that transformer-based \glspl{LLM} trained on code could generate fluent, context-aware suggestions in real-time coding workflows \autocite{TabNineBlog2019}. This marked one of the first practical demonstrations that large pretrained models could assist developers inside the editor, not just in isolated research settings.

The field saw a major leap forward with the release of OpenAI’s Codex in 2021, a Transformer-based \gls{LLM} fine-tuned on billions of lines of public source code \autocite{Chen2021}. Codex could translate natural-language instructions into executable code and complete sizable functions from brief descriptions. Its performance was evaluated on the HumanEval benchmark, a set of hand-written Python programming problems with associated unit tests designed to assess functional correctness. On this benchmark, Codex achieved 28.8\% pass@1 and 70.2\% pass@100, indicating that it could generate correct solutions on the first attempt in nearly one-third of cases, and given 100 attempts, solve over 70\% of tasks \autocite{Chen2021,Austin2021}. These scores far surpassed those of previous models and cemented Codex as a state-of-the-art tool for code synthesis.

That same year, GitHub Copilot~\autocite{Github2021CopilotAnnouncement} was launched as a commercial integration of Codex, embedding the model into popular \glspl{IDE} like VSCode. This marked a watershed moment: \gls{AI} assistants had graduated from prototype to production, entering the daily workflows of thousands of developers. Copilot could suggest entire lines or blocks of code, adapting to the context of the file and the developer's comments, essentially acting as a real-time ``\gls{AI} pair programmer.'' This breakthrough catalyzed efforts across industry and academia. DeepMind released AlphaCode, a system that tackled competitive programming problems by generating and ranking thousands of code candidates, achieving a top-54\% rank among human competitors \autocite{Li2022}. Google developed models like PaLM-Coder and Codey, while Amazon introduced CodeWhisperer, a commercial code assistant integrated with AWS developer tooling \autocite{codewhisperer2023}.

The open-source community also responded. Meta’s research division released Code Llama, a family of Transformer models fine-tuned for code generation and trained on massive code corpora, built upon their earlier LLaMA models \autocite{codellama2023}. Similarly, BigCode’s StarCoder initiative produced competitive open-weight models, democratizing access to high-quality code generation tools.

The early 2020s thus marked a turning point: \gls{AI}-powered developer tools evolved from academic experiments to integrated assistants, reshaping workflows and accelerating a shift toward natural-language-driven programming. Transformer-based models moved from being theoretical explorations to practical infrastructure embedded within the software development lifecycle.

\section{Productivity Gains with AI-Enhanced Developer Workflows}
\label{sec:productivity-gains}
The introduction of \gls{AI} coding assistants has led to measurable improvements in developer productivity, as evidenced by both controlled studies and industry surveys \autocite{Ziegler2025}. By automating boilerplate coding and offering relevant suggestions, these tools allow developers to focus on higher-level logic and design.

A notable controlled experiment by Peng et al. (2023) provided empirical evidence of substantial productivity gains: developers were tasked with building an HTTP server, and those with access to GitHub Copilot completed the task 55.8\% faster than those without \gls{AI} assistance \autocite{Peng2023}. This study, involving a treatment group using Copilot and a control group coding manually, demonstrated that \gls{AI} suggestions can significantly reduce development time for certain tasks. The assisted developers not only finished faster on average, but many also reported experiencing less frustration on routine coding steps, since the \gls{AI} handled repetitive code and syntax details.

Beyond speed, \gls{AI} assistants appear to positively impact software quality and developer satisfaction in some contexts. Microsoft's research indicates that a majority of developers feel more productive and are willing to rely on \gls{AI} for routine tasks. For instance, a large-scale survey reported that 70\% of Copilot users felt more productive, and 77\% preferred not to work without such an \gls{AI} assistant once accustomed to it \autocite{Microsoft2023Copilot}.

The integration of \gls{AI} coding assistants has introduced a paradigm shift in software development workflows. By enabling developers to articulate coding intentions through natural-language prompts, these tools facilitate the rapid generation of corresponding code snippets, effectively allowing developers to operate at a higher level of abstraction during the initial stages of development. This capability accelerates prototyping and solution exploration, as developers can quickly iterate on ideas without delving into low-level implementation details. Moreover, by automating repetitive tasks and providing contextually relevant code suggestions, \gls{AI} assistants allow for developers to concentrate on design and problem-solving aspects of their projects. This evolution from manual coding to intent-driven development has been a significant factor in the widespread adoption of \gls{AI} tools across the software industry.

\section{Limitations and Risks of AI Code Assistants}
\label{sec:ai-limitations}
Despite their impressive capabilities, current \gls{AI} code assistants come with notable limitations and risks. Productivity gains are not universal across all users or coding contexts. The same study by Peng et al. (2023) discussed in~\ref{sec:productivity-gains}, also reported on mixed outcomes alongside its findings on productivity gains: developers with more experience or those adept at prompt engineering benefitted the most, while some novices struggled to evaluate \gls{AI}-generated code for correctness. This indicates that effective use of \gls{AI} tools requires a new set of skills, such as writing good descriptive prompts and critically reviewing outputs, rather than blindly accepting suggestions \autocite{Peng2023}.

Moreover, traditional productivity metrics like lines of code written or task completion speed may not fully capture the nuanced changes in developer workflows introduced by \gls{AI} assistants. As Haque et al. (2025) point out, these conventional measures often overlook the cognitive shifts and new forms of collaboration that arise when developers interact with \gls{AI} tools \autocite{Haque2025}.

Meanwhile, the primary technical concern centers on the accuracy and correctness of code generated by \glspl{LLM}. These models can and often do produce syntactically valid yet semantically incorrect or suboptimal code, particularly when faced with ambiguous prompts or inputs that lie outside their training distribution. Unlike a human collaborator, an \gls{AI} lacks genuine understanding of a developer’s intent and instead generates code based on learned statistical patterns. As a result, it may introduce subtle bugs or flawed logic that elude initial review. Dou et al. (2024) conducted an extensive evaluation of \gls{LLM}-generated code and found that functional bugs, code that runs but fails to fulfill its intended task, were the most common error type \autocite{Dou2024}. This highlights the risk of accepting \gls{AI}-generated suggestions without rigorous testing or human oversight, as such code may appear correct on the surface while introducing latent defects into the software.

Furthermore, \gls{AI} models have a tendency to ``hallucinate''---i.e., produce outputs that are contextually plausible but factually or logically incorrect. In coding, a hallucination might manifest as a call to a non-existent API or the use of an incorrect algorithm \autocite{Tian2024}. Such mistakes require vigilant oversight by the developer, partially offsetting the productivity gains.

Security and compliance risks are another critical issue. Studies have shown that code generation models may inadvertently produce insecure code patterns. Pearce et al. evaluated Copilot’s outputs on security-sensitive programming tasks and found that a significant fraction of the suggestions contained vulnerabilities such as SQL injection, buffer overflows, or use of outdated cryptography \autocite{Pearce2022}. The \gls{AI}, having been trained on a vast corpus of public code, which itself includes insecure code, does not inherently distinguish between safe and unsafe practices. It may regurgitate common but dangerous coding patterns. This raises concerns for organizations about introducing vulnerabilities through \gls{AI}-recommended code. Moreover, since these models lack an understanding of secure coding guidelines, they will not automatically sanitize inputs or enforce security checks unless explicitly prompted to do so.

Legal and ethical risks have also been identified. \gls{AI} assistants trained on open-source code have been observed to sometimes reproduce snippets of that code verbatim in their suggestions. If those snippets are under restrictive licenses (e.g., GPL), using them without attribution could violate license terms. This issue of ``code memorization'' was highlighted soon after Copilot’s launch, leading to debate on whether \gls{AI}-generated code might inadvertently carry over copyright from its training data \autocite{Sandoval2022}. A related ethical concern is attribution, developers using \gls{AI} suggestions might not realize when a large suggested block actually comes from a specific source, potentially failing to give credit where due \autocite{Xu2024}. These uncertainties have even led to ongoing litigation challenging the legality of training on open code and the outputs produced, highlighting unresolved questions in this space.

Beyond correctness, security, and legal compliance, there are broader impacts on developer behavior to consider. Over-reliance on \gls{AI} suggestions could lead to erosion of fundamental programming skills or understanding, as developers might accept answers without fully reasoning through the problem. There is also the risk of cognitive bias: if an \gls{AI} often suggests a particular solution approach, it might narrow a developer’s exposure to alternative, potentially more innovative solutions. Recent work has also shown that developers perceive \gls{AI} assistants as confident and helpful, but sometimes blindly trust them more than traditional community-based platforms, raising concerns about misplaced confidence and reduced scrutiny of suggestions \autocite{Li2023QA}.

Additionally, initial reports indicate that indiscriminate use of \gls{AI}-generated code can increase ``code churn'' ,  developers may spend additional time later rewriting or cleaning up \gls{AI}-produced code that was not optimal in the first place, especially if it leads to integration problems with the rest of the codebase. In one study, teams using \gls{AI} assistance saw a higher rate of edits and refactors post-code-review, suggesting that some of the time saved upfront may be lost in downstream corrections \autocite{GitClear2024Copilot}.

While these tools are powerful, they remain just that, tools. A competent \gls{AI} coding assistant will not take the place of the developer, and understanding their limitations is crucial. Responsible use entails treating the \gls{AI} as a junior developer: helpful and fast, but in need of supervision, testing, and guidance to avoid the aforementioned risks.

\section{Motivations for On-Premises AI Deployment: Privacy and Compliance}
\label{sec:on-prem-motivations}
The rise of cloud-based \gls{AI} coding assistants has triggered serious privacy and confidentiality concerns, especially within enterprises handling proprietary code. Services like GitHub Copilot and Amazon CodeWhisperer rely on sending the developer’s prompt (which may include source code and descriptions of tasks) to remote servers where the \gls{AI} processing occurs \autocite{GithubCopilotArchitecture,AWSCodeWhispererPromptEngineering}. For companies, this architecture poses a risk: sensitive source code or trade secrets might be exposed to a third-party and potentially retained in logs or used to further train the \gls{AI}. Even if service providers pledge not to misuse client data, the mere transmission of code outside the corporate firewall alone is already seen as a liability in many industries.

High-profile incidents have underscored these fears. In April 2023, it was reported that engineers at Samsung accidentally leaked confidential semiconductor source code by pasting it into ChatGPT, after which the company discovered the data could no longer be retracted from OpenAI’s servers \autocite{Park2023Samsung}. This incident led Samsung to swiftly ban the use of external \gls{AI} tools like ChatGPT for any internal code or data, citing the inability to control or delete information once it leaves their network.

Samsung’s reaction is not isolated. A wave of organizations, particularly in finance and government, have instituted preemptive restrictions on cloud \gls{AI} usage. Major banks such as JPMorgan Chase, Bank of America, and Citigroup in early 2023 either forbade or heavily limited employees from inputting company data into ChatGPT or similar tools \autocite{Kessel2024}. Their concern centers on regulatory compliance and client confidentiality, any leak of personally identifiable information or strategic code could be devastating and potentially breach regulations like the GDPR or industry-specific data protection rules.

Furthermore, many jurisdictions and regulatory bodies have started scrutinizing how \gls{AI} services handle user-provided data. For example, Italy’s data protection authority briefly banned ChatGPT in 2023 until assurances about data handling were given, signaling that even government regulators view the unfettered cloud \gls{AI} data flow as problematic \autocite{Reuters2023}. These privacy concerns have fueled a broader reconsideration of \gls{AI} deployment strategies, with organizations increasingly moving away from cloud reliance toward internal, locally controlled solutions for handling sensitive data. Companies are increasingly asking whether the convenience of a hosted \gls{AI} service is worth the potential exposure of their intellectual property. As a result, demand has grown for \gls{AI} solutions that give organizations greater control. This demand is evident in new offerings by major \gls{AI} providers themselves: for instance, Microsoft announced plans for an enterprise-grade version of its Azure OpenAI Service, offering private deployment of language models like GPT-4 on dedicated Azure infrastructure. This ensures customer data remains isolated and is not used for model training \autocite{MicrosoftAzureOpenAIPrivateDeployment}. Such moves illustrate that even cloud \gls{AI} vendors acknowledge the need for data privacy safeguards to convince enterprise customers. Growing concerns over privacy and confidentiality have become a pivotal factor in prompting organizations to rethink their reliance on cloud-based \gls{AI} tools, accelerating interest in on-premises and more controllable alternatives.

\subsection{Case Studies: Transitions from Cloud to On-Premises AI Solutions}
\label{sec:case-studies}
To illustrate this trend, we highlight several concrete examples of organizations shifting from cloud-based \gls{AI} code assistants to on-premises or otherwise controlled solutions are beginning to emerge as the technology matures and concerns mount.

\begin{itemize}
	\item \textbf{Case 1:} \textbf{Samsung:} As stated above, after incidents where sensitive code was exposed via ChatGPT, Samsung not only banned external \gls{AI} platforms but reportedly began exploring the development of an internal \gls{AI} system for coding assistance, to be hosted on its own servers. This internal system would allow Samsung developers to enjoy some benefits of \gls{AI} suggestions without any code ever leaving the corporate network. While details of Samsung’s in-house solution remain private, the case exemplifies how a large enterprise pivoted in response to privacy risks: from tentative adoption of a public \gls{AI} tool to investing in a proprietary, locally hosted \gls{AI} assistant aligned with its compliance needs \autocite{sharma2023samsung}.
	\item \textbf{Case 2:} \textbf{JPMorgan Chase:} In February 2023, JPMorgan became one of the first financial institutions to restrict employees from using ChatGPT, due to concerns about sensitive financial code and data being shared externally. Instead of abandoning \gls{AI} assistance altogether, JPMorgan took a different approach: it built an internal \gls{AI} assistant named ``LLM Suite.'' By 2024, JPMorgan’s \gls{LLM} Suite was rolled out to 60,000 employees as a behind-the-firewall solution for generative \gls{AI} tasks. \gls{LLM} Suite acts as a portal to large language models in a way that, as JPMorgan’s Chief Data and Analytics Officer Teresa Heitsenrether noted, ``we can leverage the model while still keeping our data protected''~\autocite{Kessel2024}. In practice, this means the \gls{AI} (which is based on models from OpenAI but hosted in a dedicated environment) can be used to summarize documents or suggest code, yet all prompts and responses remain confined to JPMorgan’s infrastructure or a trusted cloud tenant that does not commingle data with others. This case demonstrates a transition from a general-purpose cloud \gls{AI} to a tailored on-premises (or private-cloud) \gls{AI} solution in order to meet strict compliance standards.
	\item \textbf{Case 3:} \textbf{Los Alamos National Laboratory (LANL):} In November 2023, LANL expanded its partnership with SambaNova Systems to enhance its generative \gls{AI} and large language model \gls{LLM} capabilities. This expansion included scaling up the deployment of SambaNova's DataScale systems and adopting the SambaNova Suite, a comprehensive, enterprise-optimized \gls{AI} platform designed for on-premises deployment. By hosting these solutions locally, LANL can ensure stringent data security, maintain full operational control, and comply with rigorous regulatory standards. This decision illustrates a broader trend among security-conscious organizations toward self-hosted deployments leveraging open-source models to protect data integrity and privacy \autocite{SambaNova2023LANL}.
\end{itemize}

Beyond individual companies, there is a growing shift toward on-premises \gls{AI} deployments in sectors with elevated privacy, security, or regulatory demands. Organizations in areas such as healthcare, finance, and national security are increasingly cautious about exposing sensitive data to third-party cloud services, prompting interest in self-hosted \glspl{LLM} and locally managed \gls{AI} infrastructure. In the public sector, for instance, several European governments in France and Germany have emphasized the importance of ``sovereign AI'' ,  systems that can be deployed in national data centers to ensure data autonomy and compliance with regional privacy laws. A notable example is the European GAIA-X initiative, which aims to build a federated and secure data ecosystem for cloud and \gls{AI} services within Europe’s jurisdiction. Among its goals is to promote digital sovereignty by supporting infrastructures that can be hosted entirely within a country’s own boundaries \autocite{GaiaXHub2024}. While not specific to code generation, these efforts reflect a broader movement: driven by privacy concerns and regulatory obligations, many organizations are transitioning from general-purpose, cloud-based \gls{AI} to more controlled, locally managed deployments. Though such transitions can involve performance and maintenance trade-offs, the imperative for data control remains a dominant force shaping enterprise adoption strategies.

\section{Local AI Adoption for Privacy and Compliance}
\label{sec:local-ai-for-privacy}
The push for locally hosted \gls{AI} code assistants stems from the desire to reconcile the benefits of \gls{AI} with stringent privacy and compliance requirements. In an enterprise setting, a ``locally hosted'' \gls{AI} typically means the model is run either on individual developers’ workstations, hosted on  company’s on-premises servers, or in a private cloud instance under the company’s control. This ensures that source code and prompts used for \gls{AI} never leave the trusted environment.

Over the last two years, the feasibility of such local \gls{AI} deployments has significantly improved. One catalyst was the release of powerful open-source \glspl{LLM}. For instance, Meta’s LLaMA and LLaMA 2 models \autocite{Touvron2023} demonstrated that smaller-scale (7B--70B parameter) models, when trained on large corpora including code, could achieve performance on par with some proprietary models. LLaMA’s release under a permissive license in 2023 (especially LLaMA 2 which is available for commercial use) enabled organizations to take these base models and fine-tune them on their own code repositories securely.

Following Meta’s move, other open initiatives like BigCode’s StarCoder provided highly capable code-specific models openly \autocite{Li2023StarCoder}. StarCoder, a 15-billion-parameter model trained on public GitHub code, has a performance approaching that of Codex on many tasks, and being open-source, it can be run locally or even modified to better suit an enterprise’s coding style and libraries.

Adopting a locally hosted \gls{AI} code assistant is still an experimental endeavor for most enterprises, as it comes with challenges in setup, maintenance, and ensuring quality. Unlike a managed cloud service, running an \gls{LLM} in-house requires significant computational resources (GPUs or specialized \gls{AI} hardware) and expertise in machine learning engineering. Companies venturing into this space often start with pilot projects. For example, an enterprise might set up an instance of an open-source model (like Code Llama or StarCoder) on a high-end server and integrate it with an internal \gls{IDE} plugin for developers to test. Initial trials help in assessing whether the model’s suggestions are accurate enough and whether latency is acceptable for real-time usage.

In many cases these locally hosted models need further fine-tuning or prompt engineering to specialize them for the company’s codebase or coding standards. This fine-tuning process can be conducted entirely within the organization’s own infrastructure, ensuring that proprietary code remains internal while enabling the development of a model specifically tailored to its coding practices, thereby creating their own custom model. Recent studies have shown that fine-tuning a code \gls{LLM} on a domain-specific corpus (for example, a particular company’s repositories) can improve its relevance and reduce hallucinations in that domain, though care must be taken to avoid overfitting \autocite{Roziere2023}.

From a compliance perspective, locally hosted \gls{AI} offers clear advantages. Since the entire inference process stays within the organization, there is a much lower risk of a data breach or unintended data retention by third parties. Companies in sectors like finance and healthcare, bound by regulations such as HIPAA or PCI-DSS, find this control necessary to even consider using \gls{AI} for code assistance. Additionally, hosting models locally enables auditability: the organization can log every prompt and completion, control versioning of the model, and ensure that outputs can be traced and reviewed, which is helpful for explaining decisions or conducting code audits.

Some organizations also see strategic benefits in building internal \gls{AI} capabilities, it reduces dependency on external vendors and can be tailored to internal development practices (for example, an on-prem assistant could be instructed to always enforce certain secure coding guidelines). Nevertheless, it remains experimental in the sense that the ecosystem for on-prem code \glspl{LLM} is still maturing. The rapid pace of \gls{AI} research means that open models are improving quickly, but keeping up-to-date with the latest techniques (such as model quantization for efficiency, or applying reinforcement learning from human feedback for better alignment) is non-trivial for a company whose core business is not \gls{AI}. Despite these challenges, all these developments point to locally hosted \gls{AI} code assistants becoming a viable option for enterprises that require privacy and compliance, effectively bridging the gap between advanced \gls{AI} capabilities and the strict governance policies those enterprises must uphold.

\section{Cloud vs. Local AI: Landscape and Trends}
\label{sec:ai-trends}
The competitive landscape for \gls{AI} coding assistants now spans a spectrum from fully cloud-based services to fully local solutions, with hybrid approaches in between. On one end, we have cloud-hosted proprietary models like OpenAI’s Codex (via GitHub Copilot) and Google’s Codey, which benefit from massive scale and continual improvements by specialized \gls{AI} teams. On the other end, we have open-source or self-hosted models that organizations can run independently.

A key trend in recent years is the narrowing of the performance gap between open models and proprietary ones, especially for code tasks. Research contributions such as StarCoder and Code Llama have shown that with sufficient data and tuning, open models can achieve competitive pass@1 scores on standard benchmarks \autocite{Li2023StarCoder}. For example, Code Llama (34B parameters) was reported to reach near state-of-the-art performance on the HumanEval benchmark, approaching the capabilities of OpenAI’s early Codex models.

Likewise, community-driven fine-tunes such as WizardCoder~\autocite{Luo2023} have demonstrated competitive performance. Trained through instruction tuning, where models are optimized to follow natural language prompts and generate task-relevant outputs, WizardCoder, despite being openly available, has in some programming tasks slightly outperformed proprietary models. Notably, some of these gains were achieved by incorporating filtered outputs from those same closed models. This narrowing of the performance gap has prompted more organizations to view local \gls{AI} solutions as viable alternatives rather than inferior fallbacks.

In terms of market trends, cloud \gls{AI} assistants currently enjoy widespread adoption due to ease of use. GitHub reported that as of 2023, over 50,000 organizations (including one-third of Fortune 500 companies) had adopted Copilot in some capacity \autocite{Opsera2024}. These services often integrate seamlessly with popular development environments and offer features like cloud-based model updates, team management, and telemetry that make them attractive to enterprise IT departments from a convenience standpoint.

However, the very ubiquity of cloud solutions has driven differentiation. Companies like Amazon and Google have introduced their own \gls{AI} coding assistants, Amazon Q Developer and Gemini Code Assist Enterprise respectively, not only to compete on quality but also to address enterprise data governance preferences. Amazon Q Developer, which incorporates the functionality of the earlier CodeWhisperer tool, includes a Professional tier explicitly designed for organizational use: it does not retain or use user code for model training, positioning it as a more enterprise-friendly alternative to Copilot, particularly for companies with strict data privacy requirements~\autocite{AmazonQDeveloper}. Similarly, Google's Gemini Code Assist Enterprise offers deep local codebase awareness supported by a large token context window, enabling more relevant code generation and transformation tailored to enterprise applications~\autocite{google2024gemini}.

At the same time, Microsoft and OpenAI, facing pressure from privacy-conscious clients, have begun offering private deployments (e.g., Azure OpenAI Service allowing a dedicated instance of Codex/ ChatGPT for a client) \autocite{MicrosoftAzureOpenAIPrivateDeployment}. The pricing models also differ: cloud assistants are typically subscription-based or pay-as-you-go, whereas a local solution has upfront hardware costs but potentially lower marginal cost per use if scaled. Moreover, a small development team or for individual developers locally hosting open models are a much more realistic endeavor than paying for such enterprise solutions solely for privacy concerns.

Another important aspect of the cloud vs local comparison is iteration speed and customization. Cloud providers can roll out model improvements or new features (like vulnerability filters or multiline completions) to all users at once. Local models, in contrast, give the user the freedom to customize ,  for example, a company could prioritize a local model to prefer certain coding styles or internal libraries by fine-tuning it, a level of customization that a one-size-fits-all cloud service cannot provide.

A growing trend involves hybrid deployment strategies, where organizations employ cloud-based \gls{AI} assistants for general development tasks while reserving local models for sensitive or proprietary code. This ``dual'' setup allows enterprises to balance convenience with confidentiality. Development environments are increasingly incorporating support for such configurations, enabling automatic routing of inference requests to either local or cloud endpoints based on data classification or policy constraints \autocite{PALO}.

Looking ahead, the competitive landscape will likely be shaped by upcoming regulatory changes, potentially favoring local solutions if stricter data locality laws emerge, and by technological advances that improve model efficiency, possibly making it feasible to run powerful models on a single server or even a developer’s laptop, greatly boosting the appeal of locally hosted \glspl{LLM}. In parallel, the open-source community continues to drive advancements in benchmarking and transparency. Openly released models are frequently accompanied by detailed evaluation metrics and benchmark comparisons, which can illuminate areas where proprietary systems underperform. This transparency has become a catalyst for broader improvements across both open and closed model ecosystems.

The current landscape is highly dynamic: while cloud \gls{AI} solutions still dominate in adoption and raw performance, local alternatives are advancing quickly, fueled by open innovation and growing demands for data control. Organizations now face a context-dependent decision, weighing trade-offs between compliance, cost, customization, and access to cutting-edge capabilities.

\section{Benchmarking LLMs for Code Generation and Developer Assistance}
\label{sec:llm-benchmarks}
\glspl{LLM} are typically evaluated on code generation through measures of functional correctness, where a model's outputs are validated against test cases. The predominant metric in academic literature is pass@k, the probability that at least one of $k$ generated outputs passes all test cases. This approach, first explored by Kulal et al (2019).~\autocite{kulal2019spoc} and formalized in the context of \glspl{LLM} by Chen et al (2021).~\autocite{Chen2021}, has become the standard for evaluating code synthesis models. Researchers emphasize that unlike surface-level metrics such as BLEU or ROUGE, execution-based validation directly reflects a model's utility for software development tasks~\autocite{xu2022systematic}. Consequently, most benchmarks present a natural language prompt and use predefined test suites to assess correctness, with little focus on non-functional metrics such as code style or runtime efficiency.

This section surveys benchmarks designed to evaluate \glspl{LLM} on code generation and developer assistance. Particular attention is paid to those benchmarks that are publicly available, rely on execution-based evaluation, and are most applicable to secure or locally deployed systems. Benchmarks focused solely on general-purpose natural language tasks are omitted unless their structure directly supports coding-related evaluation.

\subsection{HumanEval}
HumanEval was introduced as part of OpenAI's evaluation suite for Codex~\autocite{Chen2021}. It comprises 164 hand-written Python problems, each consisting of a function signature, a natural language specification in the form of a docstring, and an associated set of unit tests. HumanEval tasks typically require implementing a single function, with problems drawn from domains like algorithm design, string processing, and numerical computation. By design, the dataset avoids overlap with publicly available code repositories, reducing the likelihood of data leakage into pretraining corpora. The benchmark’s clarity and compact scope make it particularly suitable for assessing first-attempt correctness. Pass@1 is often cited as the most informative metric, reflecting the reliability of a model in producing correct solutions without needing to sample multiple times. Codex, for example, achieved 28.8\% pass@1 and 70.2\% pass@100 on HumanEval, demonstrating how much performance can improve when multiple completions are allowed. Because of its concise structure, HumanEval is especially practical for evaluating models deployed in local environments, where sampling may be limited due to resource constraints. Nevertheless, its Python-only scope and relatively small size have spurred the development of complementary benchmarks.

\subsection{MBPP}
The \Gls{MBPP} benchmark was created to address some of the limitations inherent in HumanEval~\autocite{Austin2021}. Comprising 974 crowd-sourced Python problems aimed at entry-level programmers, \gls{MBPP} offers greater diversity and scale. Unlike HumanEval’s docstring-style prompts, \gls{MBPP} presents natural-language problem descriptions that more closely resemble the way users might phrase queries to a code assistant. This makes \gls{MBPP} particularly relevant for evaluating models in interactive, assistance-focused contexts. Both benchmarks share a similar evaluation procedure: code outputs are validated using hidden unit tests, and performance is reported in terms of pass@k. However, \gls{MBPP} introduces more variation in linguistic formulation and includes a broader set of programming constructs, such as loops, data structures, and string manipulations. While HumanEval remains more controlled, \gls{MBPP}’s variability provides a richer testbed for assessing how well models generalize across tasks. That said, \gls{MBPP} also inherits certain limitations, most notably, its restriction to Python and the possibility that some problem solutions might appear in web-scraped training datasets.

\subsection{APPS}
Whereas HumanEval and \gls{MBPP} focus on isolated function synthesis tasks, the \Gls{APPS} benchmark expands the problem space to full program generation~\autocite{Hendrycks2021}. With 10,000 Python problems collected from competitive programming platforms such as Codeforces and LeetCode, \gls{APPS} challenges models with tasks that require parsing complex problem descriptions and assembling multi-step solutions. The evaluation remains grounded in execution-based metrics, but the shift from single-function synthesis to script-level generation introduces new difficulties in reasoning, control flow, and input/ output formatting. Notably, \gls{APPS} is organized by difficulty level, allowing researchers to examine model performance across a spectrum from beginner to expert. While simpler problems resemble those in \gls{MBPP}, the harder categories introduce dynamic programming, greedy algorithms, and graph traversal, which demand algorithmic planning and domain-specific knowledge. Even strong models, such as GPT-Neo 2.7B, have shown modest pass@1 rates of around 20\% on easier subsets, highlighting the benchmark’s difficulty. Despite its strengths, \gls{APPS} poses certain challenges for developer assistance evaluation. Generating a standalone solution script differs substantially from providing inline suggestions or contextual completions within an IDE. Thus, while \gls{APPS} reveals upper limits of generative capability, it may not reflect the day-to-day interaction patterns expected from a local coding assistant.

\subsection{CodeContests (AlphaCode)}
DeepMind’s CodeContests dataset, built to evaluate the AlphaCode system, is similar in spirit to \gls{APPS} but focuses specifically on competition-grade programming tasks \autocite{Li2022}. The dataset includes problems from contests like Codeforces and AtCoder, with evaluation centered on pass@k within a realistic submission budget, typically, 10 samples per problem. AlphaCode’s model, for example, achieved a 34\% success rate under these constraints, a notable result given the complexity of the tasks. CodeContests employs a rigorous temporal split and multiple test cases per problem, reducing risks of contamination and inflating scores. While its competitive focus makes it an excellent stress test for generalization and problem-solving, it shares \gls{APPS}’s limitations for evaluating interactive code assistance. The tasks emphasize standalone solution writing rather than incremental collaboration or partial completions. For secure, on-premise models focused on developer support, CodeContests serves as a valuable upper-bound test but may be less reflective of everyday development workflows.

\subsection{MultiPL-E}
The MultiPL-E benchmark extends the evaluation landscape by translating existing Python benchmarks like HumanEval and \gls{MBPP} into 18 programming languages~\autocite{Cassano2022}. This allows researchers to assess the cross-lingual generalization capabilities of \glspl{LLM}. Each problem maintains its original intent and is paired with language-specific test cases for evaluation. Languages span multiple paradigms, including object-oriented (Java, C\#), functional (Haskell), and scripting (JavaScript), and thus expose the syntactic and idiomatic robustness of code models. MultiPL-E's structure enables direct comparison across languages using parallel tasks. In practice, models often perform best in Python and JavaScript, languages overrepresented in training corpora, while performance may degrade in underrepresented languages. For local code assistants operating in enterprise settings, where legacy or less common languages are in use, MultiPL-E provides essential insights into model readiness. Its reliance on automatically translated tasks introduces some risk of semantic drift, but its breadth makes it a compelling complement to Python-centric evaluations.

\subsection{DS-1000}
DS-1000 was introduced to assess a distinct domain: data science and scientific computing~\autocite{Lai2022DS1000}. Sourced from StackOverflow, its 1,000 tasks target the use of libraries like NumPy, Pandas, Matplotlib, and scikit-learn. Evaluation combines execution with structural constraints, ensuring that solutions not only produce correct outputs but also use appropriate library functions and avoid hardcoded values. This multi-criteria protocol increases evaluation fidelity: Lai et al. report that only 1.8\% of accepted solutions were false positives. The problems are grounded in realistic data manipulation and analysis tasks, making the benchmark highly applicable to industry environments where such workflows are common. Unlike HumanEval or \gls{MBPP}, DS-1000 tests not just algorithmic thinking but also familiarity with specialized APIs, offering a more practical measure of a model’s readiness for data-oriented developer support. However, it remains Python-exclusive and domain-specific, serving best as a complement to more general-purpose benchmarks.

\subsection{Extended and Related Benchmarks}
Recent efforts have sought to augment existing benchmarks or explore new problem types. EvalPlus, introduced by Liu et al.~\autocite{Liu2023Rigorous}, enhances functional evaluation rigor by expanding the test suites of widely used benchmarks such as HumanEval and \gls{MBPP}. These augmented variants, HumanEval+ and MBPP+, retain the original prompts and function signatures but attach significantly more comprehensive unit tests per task. For example, HumanEval+ extends each of the 164 original problems to include over 80 test cases, synthesized through a combination of LLM-based generation and mutation strategies. \gls{MBPP}+ applies the same methodology to a filtered subset of 399 \gls{MBPP} problems with clearly defined functionality.

This augmentation addresses a key shortcoming of traditional benchmarks: sparse test coverage. With only a few test cases, models can appear accurate despite failing to generalize beyond simple inputs. EvalPlus mitigates this by enforcing correctness over a much broader input space, leading to a more reliable pass@1 metric. In doing so, it also enables the assessment of prompt stability, i.e., whether a model continues to produce valid outputs when faced with paraphrased or structurally varied inputs.

Initial studies report performance declines exceeding 10 percentage points when switching from base to augmented benchmarks, particularly for models optimized on narrow test suites~\autocite{Liu2023Rigorous}. This reveals a brittleness not captured by conventional evaluations and underscores the importance of more exhaustive functional testing, especially in high-stakes applications where first-attempt correctness is critical.

HumanEval+ and \gls{MBPP}+ now serve as valuable complements to their base versions, providing greater diagnostic depth without sacrificing comparability. Their design makes them especially relevant for secure and resource-constrained environments, where robust behavior and test-time generalization are essential.

Other recent benchmarks explore distinct challenges. HumanEvalPro and MBPP-Pro, introduced by Yu et al.~\autocite{yu2024humanevalpro}, modify existing problems to require multi-step reasoning, with intermediate outputs feeding subsequent subtasks. Results show significant accuracy drops even in advanced models, highlighting limitations in compositional problem solving. Meanwhile, CommitBench~\autocite{Schall2024CommitBench} evaluates natural-language generation for commit messages from code diffs, offering insight into adjacent software engineering tasks beyond code synthesis.

Together, these new datasets reflect a growing focus on robustness, generalization, and real-world usability. They challenge models beyond surface-level correctness and point toward richer protocols for evaluating assistance across the full software development lifecycle.

% \clearpage

\section{Summary}
\label{sec:summary}
The integration of \gls{AI} into software development has progressed rapidly over the past few years. Pre-AI tools such as linters, static analyzers, and rule-based autocompletion offered deterministic, non-generative support but remained limited to pattern recognition and diagnostics. The emergence of deep learning and transformer-based models, beginning with early prototypes like TabNine and culminating in commercial tools like Copilot, introduced a transformative shift in developer workflows. These \glspl{LLM} enabled generative capabilities, allowing developers to write code from natural language prompts and significantly accelerating routine development tasks. However, widespread adoption has raised concerns about security, licensing compliance, and consequently, legal risk. Models have been shown to produce insecure or plagiarized code, and organizations have responded with growing interest in self-hosted alternatives. Cases like Samsung’s internal ban on ChatGPT and JPMorgan’s development of an in-house \gls{LLM} platform illustrate this shift toward privacy-preserving solutions. The release of open-weight models like LLaMA 2 and StarCoder has made it increasingly viable to deploy powerful coding assistants locally, provided proper infrastructure and tuning are in place. To evaluate such models in controlled, secure environments, rigorous benchmarks are essential. Execution-based datasets like HumanEval, \gls{MBPP}, and \gls{APPS} allow for functional assessment of code generation, while benchmarks like DS-1000  and MultiPL-E explore real-world and multilingual capabilities. These evaluations inform model selection and guide fine-tuning efforts.
