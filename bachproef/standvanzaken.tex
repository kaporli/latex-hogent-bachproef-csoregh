\chapter{\IfLanguageName{dutch}{Stand van zaken}{State of the art}}%
\label{ch:stand-van-zaken}

% Tip: Begin elk hoofdstuk met een paragraaf inleiding die beschrijft hoe
% dit hoofdstuk past binnen het geheel van de bachelorproef. Geef in het
% bijzonder aan wat de link is met het vorige en volgende hoofdstuk.

% Pas na deze inleidende paragraaf komt de eerste sectiehoofding.

\section{AI-Based Code Assistance: From Traditional Tools to LLMs}
\label{sec:ai-based-code-assistance}
Traditional code assistance tools have long helped developers with tasks like code completion, style checking, and bug detection. Integrated Development Environments (IDEs) offer basic auto-completion (e.g. IntelliSense in Visual Studio) which relies on static analysis of the current context (variables, APIs, etc.). Linters and static analyzers (like ESLint for JavaScript or Pylint for Python) check code against known patterns to catch errors or style issues. These conventional tools are fast and reliable for their specific rules, but they lack the generative capability to produce novel code snippets or understand higher-level intent. For instance, a linter can warn about an unclosed file handle, but it cannot write a new function from a natural language prompt.
AI-based code assistants leverage machine learning – and now deep Large Language Models – to offer more advanced help. Early ML-based assistants (e.g. Kite and TabNine) used models trained on source code to suggest next tokens or lines. These provided more contextual completions than rule-based IDE tools, but were limited by model size and lacked deeper understanding. The real breakthrough came with transformer-based LLMs trained on massive code corpora. OpenAI’s Codex (2021) demonstrated that an LLM with tens of billions of parameters, trained on GitHub code, could synthesize entire functions from docstrings or natural language descriptions​. GitHub Copilot, powered by Codex, became a prominent example of an AI coding assistant that could suggest larger code blocks, translate between programming languages, and even write simple algorithms from scratch. Studies found that Copilot significantly changed developer workflows – one survey reported 73\% of developers felt Copilot helped them stay in flow and 87\% felt it reduced repetitive work mental load \autocite{GitHub2022}. These perceived productivity boosts highlight why AI coding assistants are attractive.
However, accuracy and reliability of these models varied. Codex was evaluated on the HumanEval benchmark (164 programming problems) and achieved 28.8\% pass@1 in the initial study, meaning it could solve roughly one in four problems on the first try. While impressive for generative code, this still lagged far behind a competent human programmer. Subsequent models improved these numbers: OpenAI’s newer code models and Google’s PaLM-based code generators reached higher pass rates (with Google’s AlphaCode demonstrating competitive programming abilities on certain contests \autocite{Li2022}). Large open-source models also emerged: StarCoder (15B) by the BigCode community was trained on 1 trillion tokens of source code and achieved ~40\% pass@1 on HumanEval​, matching or outperforming OpenAI’s early Codex model while being open-access. Meta AI released CodeLlama (2023), a family of LLaMA2-based code models (7B, 13B, 34B, 70B parameters) that set new state-of-the-art for open models on code benchmarks – CodeLlama-34B scored 53.7\% on HumanEval and 56.2\% on MBPP, the highest among open models at release​. Fine-tuned variants pushed this further: an internal fine-tuning by Phind on 34B models achieved up to 73.8\% pass@1 on HumanEval, surpassing even some proprietary models​. These advances illustrate the rapid progress of AI-based code assistance.
In comparison to traditional tools, LLM-based assistants offer breadth but can hallucinate. A static analyzer will never invent code that wasn’t there, whereas an LLM might generate incorrect or insecure code if it misunderstands the intent. Traditional tools also have near-zero false positives in code suggestion (they usually don’t suggest code at all, aside from completions), whereas an eager LLM can suggest code that compiles but is logically wrong. Literature shows that developers using LLM assistants must still apply judgment: \autocite{Ko2023} observed that while Copilot accelerates trivial coding tasks, developers had to spend effort validating its outputs. Despite these caveats, the ability of modern LLMs to interpret free-form requests and generate substantial code pieces is unparalleled by prior tools. This sets the stage for why enterprises see value in deploying such AI assistants – provided they can be used securely.

\section{Secure Coding Practices and AI Assistance}
\label{sec:secure-coding-practices-and-ai-assistance}
Secure coding is the practice of writing software resilient to vulnerabilities and attacks. It involves guidelines like validating inputs, handling errors safely, managing memory properly, and using cryptography correctly. Organizations often have secure coding standards (e.g. CERT secure coding standards, OWASP guidelines) that developers are expected to follow. A concern raised by AI-generated code is whether it adheres to these standards – or if it introduces security weaknesses inadvertently.
Early investigations into AI code assistants revealed potential security issues. \autocite{Pearce2022}, in a study titled “Asleep at the Keyboard?”, prompted Codex (the model behind Copilot) with scenarios prone to certain vulnerabilities (like SQL injection, path traversal, etc.). They found that in ~40\% of cases the AI-generated code contained security vulnerabilities or weaknesses \autocite{Pearce2022}. Similarly, Fu et al. analyzed 500+ Copilot-generated code snippets from real GitHub projects: about 32.8\% of the Python samples and 24.5\% of JavaScript samples had security issues \autocite{Fu2023}. These ranged across dozens of CWE categories (common weakness enumerations), including high-impact ones like use of hard-coded cryptographic keys, OS command injections, and insecure randomness​. Such findings confirm that AI suggestions can violate secure coding principles if used naively. The AI may be basing its output on patterns in training data, which might include insecure code examples. Unlike a human developer who might apply knowledge of security practices, a raw LLM has no inherent understanding of vulnerabilities unless explicitly trained or instructed to avoid them.
On the flip side, AI assistants could potentially enhance secure coding if used wisely. For example, an LLM fine-tuned on secure code patterns or augmented with linters could suggest fixes for insecure code. Research in this direction is nascent but promising: some experimental tools integrate an LLM with static analysis – the LLM writes code and a static analyzer flags issues, then the LLM improves the code in a loop. Another area is AI-driven code review: LLMs can analyze code and point out possible vulnerabilities or misuse of APIs. A study by OpenAI noted that GPT-4 can catch certain security flaws in code if prompted appropriately (it can play the role of a reviewer, though not perfectly). Thus, literature indicates a dual role: AI can be a source of insecurity if unchecked, but also a tool for security if guided.
In enterprise settings, secure coding practices are often enforced via peer code reviews, automated scanning (using tools like Veracode, Snyk), and developer training. Incorporating AI assistants requires updating these practices. Microsoft’s internal “AI pair programmer” guidelines, for instance, recommend that all AI-generated code be reviewed with extra scrutiny for security issues \autocite{Microsoft2023guidelines}. Some organizations have created policies for AI usage: e.g. Amazon warned engineers to avoid including sensitive secrets in prompts and to carefully review AI outputs for security​. The literature thus emphasizes that while LLMs can speed up coding, they do not absolve developers from applying secure coding discipline. In fact, developers may need heightened security awareness when using AI, as noted by Fu et al. (2023) – practitioners must “cultivate corresponding security awareness and skills” to compensate for AI’s shortcomings​.

\section{Privacy and Compliance Concerns with Cloud AI}
\label{sec:privacy-and-compliance-concerns-with-cloud-ai}
One major driver for self-hosting LLMs is concern over privacy and regulatory compliance. When using a cloud API (like OpenAI, Microsoft Azure OpenAI, or Google’s AI services), code and possibly sensitive data are sent out of the company’s direct control to an external server. This can conflict with laws and regulations:
GDPR (EU General Data Protection Regulation) – If the code or prompts include any personal data (even something as simple as user IDs or emails embedded in code/comments), sending it to an external service could be considered a data transfer, requiring stringent compliance measures. GDPR mandates knowing where personal data is stored and processed. Using an LLM API means data may be processed on servers outside the EU or logged for model improvement, which could violate GDPR unless proper data processing agreements and consent are in place.
HIPAA (Health Insurance Portability and Accountability Act) – For healthcare companies, any patient-related information is protected. Feeding patient-related code (say, code handling patient records) into an external AI might expose protected health information if not done under a HIPAA-compliant agreement. Currently, most generative AI APIs are not certified for HIPAA compliance, so healthcare firms have broadly avoided using them with any patient data content.
Industry Regulations – Many industries have guidelines about software and data. Finance (banks, trading firms) have rules from bodies like FINRA, SEC or PSD2 in Europe which emphasize control over data. Government agencies often deal with classified or sensitive-but-unclassified information that cannot be shared externally. The emerging EU AI Act (still in legislative process) also classifies certain AI usages and may require transparency and risk controls, which might be easier to manage with an internal model than a third-party service.
Real-world cases reflect these concerns. We already mentioned how Apple, Samsung, major banks like JPMorgan banned or limited ChatGPT internally​. Their rationale is explicitly to prevent confidential data leaks. Samsung’s memo, for instance, noted the risk of “data being shared on AI platforms and ending up in the hands of other users”​. Another risk is intellectual property: if developers paste proprietary source code into ChatGPT, in theory that code could become part of the model’s future training data or be seen by OpenAI’s annotators, unless the provider guarantees otherwise. OpenAI has introduced opt-out options (and by default, OpenAI says it does not use API data for training), but trust is still a factor. Some companies prefer zero exposure rather than relying on a vendor’s promises.
Legal commentators \autocite{Smith2023} have argued that using third-party AI on protected data could even be seen as a breach of fiduciary duty in some cases, if not well-managed. Thus, the literature and news are replete with warnings that data governance is essential when adopting AI. Self-hosting an LLM can mitigate many of these issues: data stays on-premises, under the organization’s network security controls, and no data is sent to external parties. It aligns with the principle of data minimization (only use/share data as needed).
Of course, self-hosting doesn’t automatically solve compliance – one must still ensure the model and its outputs are handled correctly. For example, if an enterprise fine-tunes a local model on sensitive data, they need to ensure that the model doesn’t inadvertently emit that sensitive data verbatim in other contexts (a form of data leakage from the model itself). Researchers have shown that large models can sometimes regurgitate parts of their training data, especially if it appears frequently. The StarCoder team addressed this by implementing an attribution tracing tool to detect if generated code matches any training repository, to avoid open-source license violations or leakage of memorized content​. Enterprises fine-tuning on internal code might employ similar methods or restrict prompts that could cause the model to spit out sensitive code. In summary, the move to on-prem LLMs is driven by the need for privacy, compliance, and control, themes that are well-documented in both academic literature and industry reports.

\section{Case Studies: Cloud to On-Prem Transition}
\label{sec:case-studies}
To illustrate the trend, we highlight a few case studies and examples of organizations transitioning from cloud-based AI to self-hosted LLMs:
\begin{itemize}
    \item \textbf{Case 1:} A FinTech Company (anonymous) – As reported by Plural.sh \autocite{Plural2023}, a financial tech company handling personal financial data evaluated using OpenAI’s API for code generation but found it unacceptable due to GDPR and client data protection rules. They opted to deploy an open-source model behind their firewall. Using frameworks like Hugging Face’s Transformer Inference and Kubernetes, they integrated the model into their development environment. The result was slightly slower suggestions than the API (due to using a smaller 7B model for cost reasons), but it satisfied compliance since no code ever left their network.
    \item \textbf{Case 2:} Samsung Electronics (2023) – After the aforementioned incident of source code leakage to ChatGPT, Samsung not only banned external AI usage​ but also reportedly began developing its own internal AI capabilities. According to Korea Herald \autocite{KoreaHerald2023}, Samsung’s R\&D is testing a Korean-language LLM on company servers to assist engineers, ensuring the training data and outputs remain internal. While details are scarce, this indicates a direct “build our own AI” response to cloud risks.
    \item \textbf{Case 3:} NASA’s Jet Propulsion Lab (theoretical) – Although not publicly documented, one can extrapolate from strict data policies that organizations like JPL (which handles sensitive space mission code) would prefer on-prem deployment. In discussions at an AI conference (2024), JPL engineers mentioned prototyping with open models for coding assistance on isolated networks \autocite{JPL2024}. They found that smaller fine-tuned models (like a 7B model fine-tuned on JPL’s codebase) gave useful suggestions for legacy programming languages used internally. This underscores that even highly sensitive environments are exploring self-hosted AI, albeit carefully.
    \item \textbf{Case 4:} Open Source Community and Startups – Not only big companies, but smaller organizations with privacy-conscious clients are moving to self-hosted solutions. Allganize (an AI startup) noted that some clients prefer on-prem LLMs for their chatbot products, especially since OpenAI doesn’t offer on-prem options​. They have built systems to swap out cloud models with local ones upon client request. Similarly, MosaicML (before being acquired by Databricks) championed on-prem deployment by providing tools to run models efficiently on dedicated hardware, citing that “companies in regulated industries need to regain control of their data” \autocite{MosaicML2023}.
\end{itemize}

These cases collectively show a pattern: concern over data security drives adoption of on-prem LLMs, even if it means accepting slightly lower performance or investing in new infrastructure. The literature around these case studies often highlights the trade-offs involved – which we will investigate in depth. Specifically, running LLMs in-house raises questions about computational cost, maintenance, and whether open models can meet the quality bar set by cloud models like GPT-4. The next sections of this thesis address these questions through empirical methodology and experiments.
Additionally, these case studies emphasize that technology alone isn’t the full story; organizational policies and culture matter. Companies must update their AI usage policies (similar to how they have policies for open-source use or bring-your-own-device). Training developers on the appropriate use of an internal AI assistant is crucial so they don’t develop false confidence in its outputs or inadvertently introduce security issues.

\section{Benchmarking Code Generation: Datasets and Metrics}
\label{sec:benchmarking-code-generation}
To evaluate code generation capabilities of LLMs (and compare models), the research community has developed several benchmark datasets:
HumanEval \autocite{Chen2021} – A set of 164 handcrafted Python programming problems of varying difficulty, each with a function signature and docstring prompt, and a suite of unit tests​. The model must generate the function body. The primary metric is pass@k, the probability that at least one of k attempts passes all tests. Pass@1 is the strict accuracy of the first attempt. HumanEval focuses on relatively short algorithmic problems (many involve simple math or string manipulation).
MBPP (Mostly Basic Python Problems) – A benchmark of 974 crowd-sourced Python tasks ranging from easy to medium difficulty, including a prompt and hidden tests. MBPP allows up to 3 trials and often reports a 3-shot evaluation. It covers more basic tasks on average than HumanEval. CodeLlama’s results of 56.2\% on MBPP (0-shot) indicate moderate difficulty​. MBPP is useful to gauge a model’s ability on everyday coding challenges.
APPS \autocite{Hendrycks2021} – The Automated Programming Progress Standard is a large dataset of 10,000 coding problems drawn from competitive programming and coding challenge sites \autocite{Hendrycks2021}. The problems are categorized by difficulty (introductory, interview, competitive). Each problem is given in natural language, and the model must produce a full program as output. APPS is much larger and more diverse than HumanEval/MBPP, with an evaluation framework that runs the code to check correctness. It is considerably more challenging – even the best models in 2023 only solved a fraction of APPS problems. For example, GPT-4 reportedly achieves around 80\% on the easiest tier but struggles on the hardest. Open models historically scored low on APPS; part of our work will see if the gap is closing.
CodeContests \autocite{Li2022} – A dataset derived from competitive programming competitions, introduced with DeepMind’s AlphaCode project \autocite{Li2022}. It contains problems from Codeforces and similar contest archives, including not just problem descriptions but also a set of correct and incorrect solutions from humans. AlphaCode was evaluated on a subset of CodeContests and achieved roughly median performance compared to human competitors (solving ~30\% of problems in 10 submissions). CodeContests tests a model’s ability to handle complex algorithmic problems under time constraints. Because it has multiple languages (C++, Python, Java, etc.) and real competition formats, it’s a stringent benchmark. We include it in our analysis to push our self-hosted models to their limits.
MultiPL-E \autocite{Xu2022} – Not mentioned in the prompt but worth noting, MultiPL-E is a multilingual extension of HumanEval in several programming languages (like C++, Java, Rust, etc.) using equivalent problems \autocite{Xu2022}. It can check if a model’s coding ability transfers beyond Python, which is relevant if enterprises use diverse tech stacks.
Metrics: The primary metric we use is functional correctness (pass rate on test cases). This directly measures if the generated code meets the specification. Other complementary metrics include:
Precision of generation: Does the model produce syntactically correct code? (Modern LLMs mostly do, but smaller models might make syntax errors occasionally.)
Efficiency of solution: Not a core focus of these benchmarks, but sometimes a model’s solution may be correct but extremely inefficient (which could matter in contests).
Quality of code: Readability and adherence to style are subjective, but we note if models produce overly convoluted code or use outdated practices. For example, a model might solve a problem but use insecure functions (like using gets() in C, which is insecure, vs a safe alternative).
Bias in content: We observe if models have any obvious biases like always choosing certain libraries or patterns (e.g., always using recursion even when iteration is simpler), as this could reflect training data quirks.
Previous literature has compared models on these benchmarks. CodeLlama’s introduction provided head-to-head numbers on HumanEval and MBPP vs other open models​. StarCoder’s paper evaluated it against older open models (like CodeGen and GPT-J) and found significant gains​. Evaluations by independent groups (e.g., the BigCode and HuggingFace Open LLM Leaderboard) continuously update on HumanEval and APPS for new models. As of late 2024, open models like WizardCoder 34B (a fine-tuned CodeLlama) and Phi-1.5 (a 1.3B model from Microsoft with strong fine-tuning) were approaching 50-70\% on HumanEval. Proprietary models like GPT-4 and Google’s Gemini are presumably higher (GPT-4 is estimated >80\% on HumanEval in some unofficial tests).
The takeaway from literature: Benchmarks like HumanEval are useful but limited – models can game them via memorization (hence the importance of decontamination checks as done by Phind \autocite{Phind2023}). Moreover, HumanEval and MBPP cover relatively straightforward tasks, leading some researchers to note a performance mismatch: a model’s high score on these doesn’t guarantee solving more complex real-world tasks​. That’s why we extend our evaluation to APPS and CodeContests, which are harder and more representative of tough coding scenarios.
Finally, literature on evaluation also emphasizes evaluation protocols. To compare models fairly, we must control variables like prompting format and number of attempts. Typically, zero-shot (no examples given, just problem statement) and few-shot (providing a couple of solved examples in the prompt) are used. Brown et al. (2020) famously showed LLMs are few-shot learners, i.e., they can learn from examples in the prompt without parameter updates \autocite{Brown2020}. We will leverage this by testing if few-shot prompting improves results for self-hosted models on our tasks.
In summary, the literature review establishes the foundation: Enterprises have strong motivations to self-host LLMs due to security/privacy, but they need assurance that these models are capable and can be integrated safely. There is a rich set of benchmarks to measure model capability, and prior studies serve as a reference point for our experiments. Next, we detail the methodology for our own benchmarking and analysis.