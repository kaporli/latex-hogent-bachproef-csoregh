%-------------------------------------------------------------------------%
%  Proof-of-Concept Overview                                              %
%-------------------------------------------------------------------------%
\chapter{Proof of Concept}
\label{ch:proof-of-concept}

In this proof of concept, we present a structured evaluation of modern self-hosted language models for code generation, motivated by growing enterprise demand for privacy-preserving developer tools. The study compares leading open-weight models in a containerized, fully reproducible benchmarking pipeline, and assesses their viability as on-prem alternatives to proprietary cloud-based solutions such as GitHub Copilot or ChatGPT. Emphasis is placed not only on accuracy (via functional correctness benchmarks), but also on practical deployment characteristics: licensing, resource demands, inference flexibility, and output format compatibility. The end-to-end pipeline is built around the \texttt{bigcode-evaluation-harness}, with all experiments conducted inside a GPU-accelerated Docker container to ensure isolation and determinism. %Technical setup details, including ROCm support and hardware tuning, are documented separately in Appendix~A. The following sections justify our platform choice, model selection, and benchmarking design.

%=========================================================================%
\section{Justification of Approach}
\label{ch:poc-justification}
%-------------------------------------------------------------------------%
\subsection{Model Deployment}
\label{subsection:model-deployment-justification}

%====================  MODEL DEPLOYMENT PLATFORM  ====================%

To support the deployment of locally hosted language models, we evaluated two viable platforms: the Hugging Face Transformers library, an industry-standard toolkit for loading, configuring, and running open-weight language models via Python and the Ollama runtime, a lightweight framework designed for launching \glspl{LLM} through a simplified CLI-based workflow. Both platforms satisfy a foundational requirement of our project, offline inference with no data leaving the local environment, making them suitable for privacy-conscious developer use cases. However, they represent fundamentally different approaches to deployment: HuggingFace emphasizes flexibility and fine-grained control through programmatic interfaces, while Ollama prioritizes simplicity and accessibility. Given our need to automate large-scale benchmarking and ensure full transparency and reproducibility of runs, it was essential to weigh each platform’s strengths and limitations systematically. Table~\ref{tab:platform-checklist} summarizes this comparative evaluation across a set of practical criteria, each reflecting a technical or operational requirement relevant to our research objectives.

\begin{table}[ht]
	\centering
	\caption{Checklist of requirements for local code-model deployment.}
	\label{tab:platform-checklist}

	\renewcommand{\arraystretch}{1.2}      % row height
	% One *single* tabularx:
	\begin{tabularx}{\textwidth}{@{}X
		>{\centering\arraybackslash}p{3cm}
		>{\centering\arraybackslash}p{3cm}@{}}
		\toprule
		\textbf{Requirement}                       & \textbf{Hugging Face} & \textbf{Ollama} \\
		\midrule
		Native access to the latest models         & \checkYes             & \checkNo        \\
		Run natively on AMD ROCm (via PyTorch)     & \checkYes             & \checkNo        \\
		Fine-grained control of inference settings & \checkYes             & \checkNo        \\
		Deterministic output and version pinning   & \checkYes             & \checkNo        \\
		Programmatically parsable output           & \checkYes             & \checkNo        \\
		Minimal install effort                     & \checkYes             & \checkYes       \\
		\bottomrule
	\end{tabularx}
\end{table}


%====================== JUSTIFICATION PER REQUIREMENT ======================%
\paragraph{Native access to the latest models}
One of the key differences between Hugging Face Transformers and Ollama lies in their respective access to state-of-the-art model checkpoints. Hugging Face provides a vast and actively maintained model hub, where new releases from both research institutions and commercial developers are published frequently, often within days of their official announcement. This includes all major models discussed in our literature study, such as StarCoder, CodeLlama, and WizardCoder, which are readily available in their original formats (e.g., FP16, BF16) without the need for additional conversion or repackaging. In contrast, Ollama requires that models be pre-converted into the GGUF format and hosted in its own limited registry. While community conversions do exist, they often lag behind the latest model releases and may not include metadata essential for accurate inference (e.g., tokenizer vocab alignment, EOS token placement). This friction disqualifies Ollama as a primary platform in research settings where keeping up with the cutting edge is a prerequisite.

\paragraph{Run natively on AMD ROCm (via PyTorch)}
Given our compute environment includes an AMD Radeon GPU, native ROCm support was a non-negotiable requirement. Hugging Face Transformers, when paired with PyTorch ROCm builds, enables native GPU acceleration under Linux and WSL2, unlocking hardware-specific features like bfloat16 tensor cores and multi-stream compute. This is crucial not only for benchmarking performance (latency, throughput) but also for ensuring that model behavior under accelerated conditions remains reproducible. Ollama, in contract, builds upon llama.cpp, which at the time of writing lacks official ROCm support and primarily targets NVIDIA and CPU execution. Although some community forks have experimented with HIP integration, these are unofficial and unstable, making Ollama unsuitable for consistent ROCm-based deployments. As a result, only Hugging Face fulfilled this requirement out of the box.

\paragraph{Fine-grained control of inference settings}
Effective evaluation of code-generation models hinges on the ability to configure decoding parameters with precision. Parameters such as sampling temperature, top-$k$ and top-$p$ filtering, repetition penalties, maximum generation length, and custom stop sequences can substantially influence output quality, particularly on constrained tasks like HumanEval. The use of custom stop sequences is especially critical when working with instruction-tuned models, which often append additional text (e.g., closing prompts or signatures) unless explicitly instructed to halt generation. Hugging Face Transformers provides comprehensive access to these settings through its generation API, facilitating consistent control over generation behavior across evaluations. This level of granularity is indispensable for achieving reproducible results and conducting ablation studies that isolate the impact of specific inference strategies.

By contrast, Ollama adopts a more opaque approach to inference. While it supports a limited subset of parameters through command-line flags or its HTTP interface, many advanced generation settings are either unavailable or difficult to modify dynamically. This limitation creates a significant barrier for automated evaluation workflows that depend on programmatic control of decoding configuration, such as those implemented in benchmark frameworks like \texttt{lm-evaluation-harness} or \texttt{bigcode-evaluation-harness} (Section~\ref{sec:pipeline}). Since these frameworks rely on direct integration with libraries like \texttt{transformers} to orchestrate controlled batch evaluation, Ollama’s architecture introduces compatibility issues that hinder its applicability in rigorous benchmarking contexts. The absence of standard interfaces for setting inference parameters at runtime further complicates integration with reproducibility-focused pipelines and limits its utility in comparative experiments.

\paragraph{Deterministic output and version pinning}
To make our benchmarking process scientifically valid, we needed all results to be both deterministic and attributable to a specific software stack. Hugging Face enables this by allowing explicit pinning of library versions, e.g., \texttt{transformers==4.39.3}, \texttt{torch==2.2.1+rocm}, and by supporting fixed random seeds for all generation calls. When these controls are in place, identical inputs yield identical outputs across different machines, assuming the same compute backend. This is critical when comparing models across multiple benchmarks, or when regenerating outputs post-review. Ollama, being more of a ``black box'', hides many of its internal dependencies and updates its containerized runtimes automatically in the background. Although this makes for a smoother end-user experience, it reduces transparency and risks breaking reproducibility between test runs, particularly problematic in research or regulated enterprise environments.

\paragraph{Programmatically parsable output}
For seamless integration with our benchmarking infrastructure, it was imperative that generated outputs be captured in a structured and machine-readable format. Hugging Face allows programmatic access to raw model outputs via Python objects, making it trivial to format completions into JSON lines or pipe them directly into downstream evaluation tools like \textttbreak{lm-evaluation-harness} and \textttbreak{bigcode-evaluation-harness} (Section~\ref{sec:pipeline}). This design supports filtering, tagging, and aggregating completions with metadata like prompt IDs or timestamped seeds. Ollama, however, defaults to streaming its outputs to STDOUT, designed for interactive command-line use rather than batch processing. While it is possible to capture and post-process this output, doing so introduces brittle shell-based parsing or requires the use of its REST API, which lacks standardization across model versions. For our purposes, Hugging Face was the only platform that offered the level of integration and control necessary for a scalable and automatable evaluation pipeline.

\paragraph{Minimal install effort}
Despite its complexity and configurability, Hugging Face remains relatively light\-weight in terms of setup: installing \texttt{transformers}, \texttt{torch}, and a few supporting packages via pip is sufficient to run most models. For users already familiar with Python-based environments and virtual environments, this installation process is both predictable and easily scripted for Docker-based deployment. Ollama, by contrast, aims to minimize all configuration overhead. It can be installed with a single command and invoked immediately without writing a single line of code. From a usability standpoint, this is highly commendable, and for non-technical users or quick local tests, Ollama excels. However, in our case, since we already required containerization, scripting, and integration with evaluation tooling, the additional install effort associated with Hugging Face did not pose a disadvantage and was offset by its flexibility.

\paragraph{Conclusion}
While Ollama excels at simplicity and quick experimentation, its design targets ease-of-use over configurability. For our purposes, namely rigorous evaluation of code generation models in a reproducible, research-grade setting, it proved insufficient. Key shortcomings included its limited control over inference parameters, lack of support for deterministic runs via version pinning, and the absence of direct compatibility with established evaluation pipelines such as \texttt{lm-evaluation-harness} and \texttt{bigcode-evaluation-harness}. Moreover, Ollama's opaque runtime and console-based output make it difficult to extract structured data needed for automated benchmarking. In contrast, the Hugging Face Transformers library offered a far more comprehensive solution: full access to the latest model checkpoints, seamless integration with our tooling, and fine-grained control over every aspect of inference. These features made it not just a more flexible platform, but the only viable choice for ensuring that our findings could be reproduced, extended, and trusted within an academic or enterprise context.

%-------------------------------------------------------------------------%
%  Model Selection                                                         %
%-------------------------------------------------------------------------%
\subsection{Model selection}
\label{subsection:model-selection}
In order to accurately assess the current state of locally hosted, open-weight language models for code generation, we established a clear and pragmatic selection strategy. This strategy was developed in close collaboration with the co-supervisor of this thesis, Mr.~Leflon, whose professional background in cybersecurity informed the filtering process for secure enterprise deployment. Drawing on his industry insights, we formulated a concrete set of requirements (see Table~\ref{tab:model-reqs}) that reflect the operational needs and constraints typical of real-world on-prem environments. These include licensing conditions that allow unrestricted commercial use, parameter limits for single-GPU inference, compatibility with quantization workflows, long context window support, architectural variety, and sufficient performance on practical code generation tasks.

The goal was  to identify a diverse yet representative subset of 7B-scale models that exemplify current best practices in open-access LLM development.  Many open-weight code models were reviewed and ultimately excluded, either due to outdated architecture, insufficient benchmark performance, limited community support, or restrictive licensing. Notably, we rejected WizardCoder (due to a lack of strong 7B variants), Magicoder (largely derivative of older CodeLlama backbones), and OpenHermes-Code (minimal evaluation and ecosystem traction). Smaller models like Replit Code-v1.5 were excluded based on sub-threshold parameter count and constrained capability. Similarly, general-purpose LLaMA 2 variants and forks like CodeUp or OpenChat-Code were not considered competitive with more recent instruction-tuned or code-specialized models released in 2023-2024.

This curated list ensures that our evaluation remains forward-looking, reproducible, and aligned with the practical demands of deploying secure, in-house language models. Moreover, by establishing a common benchmark foundation, we can compare pass@1 performance across the cohort and perform a fair preliminary ranking to further narrow down candidates for deeper evaluation in downstream tasks.

\begin{table}[H]
	\centering
	\caption{Objective requirements for selecting an on-prem code model.}
	\label{tab:model-reqs}
	\renewcommand{\arraystretch}{1.2}

	%--------------------------------------------------------------------%
	\begin{tabularx}{\textwidth}{@{}>{\centering\arraybackslash}p{0.8em}%
		>{\raggedright\arraybackslash}p{3.8cm}%
		X@{}}
		\toprule
		\textbf{\#} & \textbf{Requirement}                          & \textbf{Why it matters for an on-prem assistant}                                    \\
		\midrule
		1           & Permissive license                            & Allows commercial deployment and avoids legal ambiguity                             \\[2pt]
		2           & \leq{}7B parameters                           & Enables single-GPU inference (\leq{}20GB VRAM)                                      \\[2pt]
		3           & Instruction-tuned                             & Supports natural language prompts, essential for code assistance use cases          \\[2pt]
		4           & Minimum context window size (\geq{}4k tokens) & Supports single-file and function-level code completion and moderately long prompts \\[2pt]
		5           & Quantisation-ready                            & Supports 4-8-bit precision for laptop/server deployment                             \\[2pt]
		6           & Competitive pass@1                            & Ensures sufficient real-world coding competence                                     \\[2pt]
		7           & Architectural diversity                       & Reduces overfitting to one model family; tests generalizability                     \\[2pt]
		8           & Active community                              & Ensures ongoing updates, support, and fine-tuning recipes                           \\[2pt]
		\bottomrule
	\end{tabularx}
\end{table}

After filtering with these criteria, we selected seven models that together form a rigorous, architecture-diverse benchmark cohort: \texttt{StarCoder2-7B}, \texttt{CodeGemma-7B-Instruct}, \texttt{CodeLlama-7B-Python}, \texttt{Qwen2.5-Coder-7B-Instruct}, \texttt{DeepSeek-Coder-7B-Instruct-v1.5}, \texttt{Mistral-7B-Instruct-v0.3}, and \texttt{Mamba-Codestral-7B-v0.1}. These models span different pretraining strategies, base architectures (e.g., LLaMA, Mistral, Gemma, Qwen), and instruction-tuning approaches. All of them support quantization, are self-hostable, and meet the resource constraints defined for single-GPU deployment in enterprise settings.

\paragraph{Permissive license}
All selected models are released under licenses that permit self-hosted commercial deployment, a strict requirement for enterprise integration. Most, such as \texttt{Qwen2.5-Coder-7B-Instruct}, \texttt{Mamba-Codestral-7B}, \texttt{Mistral-7B-Instruct-v0.3}, and \texttt{DeepSeek-Coder-7B-Instruct-v1.5}, are covered by permissive open-source licenses such as Apache 2.0 or MIT, which allow modification, redistribution, and private usage without complex legal barriers. Although \texttt{CodeLlama-7B-Python} and \texttt{CodeGemma-7B-Instruct} operate under more restrictive terms (Meta’s Llama 2 Community License and Google’s Gemma License, respectively), both explicitly allow internal commercial use and model hosting, which satisfies the legal needs of on-prem enterprise workflows.

\paragraph{\(\leq\)7B parameters}
All seven models contain approximately 7 billion parameters, a scale that maximizes the tradeoff between performance and resource efficiency for the hardware used during testing. This ensures compatibility with single-GPU setups equipped with high-memory consumer or workstation-grade GPUs (e.g., NVIDIA RTX 3000/4000/5000-series or AMD RX 7900 XT/XTX), enabling local deployment even in mid-sized organizations without data center-level infrastructure, an explicit requirement derived from the practical constraints of this thesis's co-supervisor’s operational environment. Models exceeding this size, such as WizardCoder-15B or Falcon-40B, were excluded due to their prohibitive memory footprint, which is incompatible with our single-GPU requirement.

\paragraph{Instruction-tuned}
Each selected model is explicitly instruction-tuned for conversational or code-assistant use cases. \texttt{Mamba-Codestral-7B}, \texttt{Qwen2.5-Coder-7B-Instruct}, \texttt{DeepSeek-Coder-7B-Instruct-v1.5}, and \texttt{CodeGemma-7B-Instruct} have undergone supervised fine-tuning on high-quality code tasks framed as natural language prompts. \texttt{Mistral-7B-Instruct-v0.3} generalizes instruction-following for a broad range of reasoning and logic-based tasks. This tuning enables models to reliably respond to developer queries, whether generating function bodies, rewriting legacy code, or providing inline explanations, without requiring prompt engineering or templating.

\paragraph{Minimum context window}
Several models in our selection exceed the baseline 4\,K token window required for file‐level tasks: \texttt{Mamba-Codestral-7B} supports up to a remarkable 256\,K tokens; \texttt{Qwen2.5-Coder-7B-Instruct} provides 128\,K tokens, enabling advanced cross‐file retrieval workflows; \texttt{Mistral-7B-Instruct-v0.3} extends to 32\,K tokens; and \texttt{StarCoder2-7B} offers 16\,K via sliding‐attention, ideal for mid‐sized source files. In contrast, \texttt{CodeLlama-7B-Python} and \texttt{CodeGemma-7B-Instruct} maintain the standard 8\,K token window, while \texttt{DeepSeek-Coder-7B-Instruct-v1.5} remains at 4\,K tokens, perfectly suited to single‐file or function‐level completions. A minimum 4\,K token context is critical for unit‐test generation, file‐wide refactoring, and retrieval‐augmented generation in enterprise codebases.

\paragraph{Quantization-ready}
All models are available or compatible with 4-bit and 8-bit quantized formats, including GGUF and \texttt{bitsandbytes} variants. Quantization allows real-time inference on consumer hardware and significantly reduces memory and compute costs. Community-driven tooling ensures that models like \texttt{StarCoder2-7B}, \texttt{Qwen2.5}, and \texttt{Mistral-7B-Instruct} are easily portable. This deployment flexibility is critical for organizations that must operate in secure environments, such as air-gapped systems with no external network access, or on remote edge devices requiring fully local inference.

\paragraph{Competitive pass@1}
The selected models demonstrate state-of-the-art performance on HumanEval, MBPP, or equivalent code benchmarks. \texttt{Mamba-Codestral-7B} exceeds 70\% pass@1 on HumanEval, while \texttt{Qwen2.5-Coder-7B-Instruct} and \texttt{DeepSeek-Coder-7B-Instruct-v1.5} achieve competitive results often surpassing older LLaMA-based baselines. \texttt{CodeGemma-7B-Instruct} and \texttt{CodeLlama-7B-Python} perform well in Python-centric benchmarks, which remain central in enterprise automation and data workflows. Pass@1 performance was a minimum bar to inclusion, as it reflects functional correctness without the need for excessive sampling or reranking.

\paragraph{Architectural diversity}
The final cohort includes models from five distinct architectural lineages: LLaMA-based models (\texttt{CodeLlama}, \texttt{Mistral-7B}, DeepSeek variants), the Qwen transformer family (\texttt{Qwen2.5}), Google’s Gemma series (\texttt{CodeGemma}), and the Mamba linear attention architecture (\texttt{Mamba-Codestral}). This variety ensures our evaluation captures behavior across differing design philosophies, positional encodings, tokenization strategies, and attention mechanisms. It also mitigates bias in task performance and improves the generalizability of our findings to future models.

\paragraph{Active community}
Every selected model benefits from significant community activity, including ongoing development, quantization forks, training insights, and issue tracking. \texttt{Qwen2.5} and \texttt{Mistral-7B} are widely adopted across fine-tuning pipelines; \texttt{StarCoder2} is supported by BigCode’s open community; \texttt{DeepSeek} models are backed by frequent updates; and \texttt{Mamba-Codestral} benefits from Mistral’s rapid release cadence. In contrast, excluded models like OpenHermes or Magicoder lack sustained development or reproducibility guarantees. Community engagement ensures long-term viability, which is critical for internal enterprise tooling.

\paragraph{Conclusion}
Together, the seven selected models comprehensively satisfy the requirements defined in Table~\ref{tab:model-reqs}. Their licensing terms, compute footprint, instruction-following capabilities, benchmark performance, and support ecosystems make them ideal candidates for secure, self-hosted code generation in professional software environments. This curated benchmark group enables us to evaluate how far open-weight models have progressed in meeting the needs of real-world enterprise development workflows.
%-------------------------------------------------------------------------%
%  Evaluation Pipeline (lm-evaluation-harness)                            %
%-------------------------------------------------------------------------%
\subsection{Evaluation Pipeline}
\label{sec:pipeline}

To evaluate the models outlined in~\ref{subsection:model-selection} on code-generation benchmarks, we leverage the \texttt{bigcode-evaluation-harness}, a benchmarking suite developed by the BigCode project specifically for assessing code-oriented language models \autocite{Bigcode}. This framework extends the more general-purpose \texttt{lm-evaluation-harness} created by EleutherAI~\autocite{EleutherAI}, which offers limited support for code-related benchmarks such as HumanEval and MBPP, but is primarily designed for \gls{NLP} tasks like MMLU, GSM8K, and HellaSwag. While both harnesses are extensible, the BigCode implementation provides out-of-the-box support for a broader set of code-specific tasks, including HumanEval+, HumanEval Instruct, DS-1000, and APPS, using test-suite execution and function-level correctness as core evaluation metrics. These metrics align closely with the practical demands of software development. Although EleutherAI’s framework can be extended to support such tasks, doing so would require significant engineering effort to replicate the domain-specific evaluation logic and test infrastructure already built into the BigCode harness.

Both harnesses are designed to interface seamlessly with Hugging Face Transformers, but the BigCode evaluation suite introduces additional infrastructure tailored to code-centric workflows. Notably, it includes preconfigured stop sequences for consistent code formatting, execution scaffolds for running test cases, and secure evaluation tools that mitigate risks such as arbitrary code execution. These capabilities are essential when assessing instruction-tuned code models, where incomplete or overly verbose generations can distort evaluation outcomes. Furthermore, the BigCode harness integrates natively with the \texttt{accelerate} library to facilitate multi-GPU inference and includes containerized deployment recipes for streamlined experimentation. By contrast, while EleutherAI’s \texttt{lm-evaluation-harness} supports local and API-based execution of Hugging Face models, its focus remains on general \gls{NLP} tasks and it lacks the specialized defaults and performance-oriented enhancements that are critical for rigorous evaluation of code generation.

Importantly, both frameworks support key configuration controls such as fixed random seeds, adjustable sampling strategies (e.g., temperature, top-$k$, top-$p$), and output logging. However, \texttt{bigcode-evaluation-harness} streamlines reproducibility by encapsulating these settings within a unified evaluation interface and aligning outputs with the expected schema for each benchmark. It also produces structured JSON artifacts, including model generations, reference completions, and scoring results, in a consistent, machine-readable format, which facilitates downstream analysis and integration with custom evaluation pipelines.

A sample CLI usage of \texttt{bigcode-evaluation-harness} is shown below:

\begin{listing}[H]
	\begin{minted}[linenos, breaklines, fontsize=\small]{bash}    
    accelerate launch main.py \
        --model <model_name> \
        --precision bf16 \
        --tasks humaneval,humanevalplus,mbpp,mbppplus \
        --do_sample False \
        --n_samples 1 \
        --batch_size 1 \
        --save_generations \
        --save_references \
        --allow_code_execution

\end{minted}
	\caption{Example usage of the BigCode evaluation harness to evaluate multiple models.}
	\label{lst:bceh}
\end{listing}

This command generates code samples for each benchmark task, executes unit tests, and logs results for each model configuration.

In summary, while both \texttt{bigcode-evaluation-harness} and \texttt{lm-evaluation-harness} offer extensible benchmarking infrastructure, \texttt{bigcode-evaluation-harness} provides a significantly lower barrier to evaluating code models due to its native support for unit-test-based code tasks, integration with code-specific formatting and execution settings, and compatibility with local deployment. Its self-contained design and focus on reproducibility made it the more appropriate choice for our privacy-sensitive, on-premise benchmarking pipeline.

%-------------------------------------------------------------------------%
%  Benchmark Selection                                                    %
%-------------------------------------------------------------------------%
\subsection{Benchmark Selection}
\label{sec:benchmarks}

A core objective of this study is to evaluate locally hosted \glspl{LLM} in their role as coding assistants, specifically in generating correct and usable code snippets for developer tasks. To support this goal, we prioritized benchmarks that assess functional correctness on well-scoped programming problems, rather than measuring broader capabilities like full program synthesis or documentation generation. Based on our literature study (Section~\ref{sec:llm-benchmarks}) and the set of officially supported tasks in the \texttt{bigcode-evaluation-harness}, we selected both the original HumanEval and \gls{MBPP} benchmarks, as well as their respective augmented variants: HumanEval+ and \gls{MBPP}+, developed under the EvalPlus initiative~\autocite{Liu2023Rigorous}. Together, these four benchmarks provide a layered evaluation strategy that balances legacy comparability with modern diagnostic depth.

\paragraph{HumanEval and HumanEval+}
HumanEval is a widely used benchmark comprising 164 manually written Python problems~\autocite{Chen2021}. Each task consists of a function signature, a descriptive docstring, and a hidden suite of unit tests for automatic correctness validation. It provides a compact but rigorous setting for evaluating first-attempt code synthesis, an important requirement for offline deployments, where iterative sampling may not be feasible.

However, as highlighted in recent literature~\autocite{Liu2023Rigorous}, HumanEval’s limited number of test cases per task can lead to inflated pass@1 scores, particularly for models prone to overfitting on surface-level patterns. To address this, we also adopted HumanEval+, which preserves the original prompts but augments each with over 80 unit tests generated via a combination of LLM-based synthesis and mutation strategies. This significantly strengthens the reliability of performance metrics, especially in scenarios demanding real-world robustness.

\paragraph{MBPP and MBPP+}
The \gls{MBPP} benchmark extends the evaluation landscape with 974 crowd-sourced Python problems, each framed as a natural-language instruction and paired with hidden test cases~\autocite{Austin2021}. MBPP’s prompts more closely resemble how developers describe problems in practice, making it especially relevant for assessing instruction adherence and contextual interpretation in coding assistants.

To mitigate concerns about narrow test coverage, we included MBPP+, an augmented subset of 399 tasks selected for functional clarity and extended with significantly larger test suites~\autocite{Liu2023Rigorous}. The augmented test cases enforce correctness under a more diverse set of inputs and edge cases while preserving MBPP’s original problem format. As with HumanEval+, this allows us to probe not only functional correctness, but also prompt stability, whether a model’s behavior remains consistent when task phrasing or test inputs vary.

\subsubsection*{Summary}
Together, HumanEval and \gls{MBPP} offer historical continuity, linguistic diversity, and problem-scale variation, while HumanEval+ and \gls{MBPP}+ improve diagnostic resolution through denser test coverage. Evaluating models across both base and augmented variants allows us to distinguish between models that are merely prompt-aligned and those that generalize reliably. This combination is particularly suited to our investigation of secure, on-premise deployment, where first-pass correctness and robustness to prompt variability are critical performance attributes. By integrating EvalPlus into our evaluation, we provide a more stringent and realistic benchmark framework aligned with enterprise coding assistant requirements.

\clearpage

\section{Practical Setup and Execution}

To validate our benchmarking strategy for evaluating instruction-tuned code generation models, we implemented a containerized, GPU-accelerated pipeline built around the BigCode evaluation harness. Our implementation targets execution on ROCm-compatible AMD GPUs running under WSL2, leveraging local model caching for reproducibility and performance. Below are the key steps:

\subsection{Dockerfile}

\begin{minipage}{\textwidth}
	\captionsetup{type=listing}
	\caption{Optimised \texttt{Dockerfile} for ROCm-based benchmarking container}
	\label{lst:dockerfile}

	\begin{minted}[linenos,
                    breaklines,
                    breakanywhere=true,
                    fontsize=\footnotesize]{docker}
                    ###############################################################################
                    #  Ubuntu 24.04 · ROCm 6.3.4 · PyTorch 2.4 · BigCode
                    ###############################################################################
                    FROM ubuntu:noble
                    LABEL maintainer="Elias Csoregh <elias.csoregh@hogent.student.be>"

                    ARG  ROCM_VER=6.3.4
                    ARG  TORCH_VER=2.4.0

                    ENV  DEBIAN_FRONTEND=noninteractive
                    ENV  PATH=/opt/venv/bin:$PATH
                    WORKDIR /tmp

    \end{minted}
\end{minipage}

This initial block defines the foundational build instructions for the container. The \texttt{FROM ubuntu:noble} directive specifies the base image, selecting \texttt{Ubuntu 24.04 “Noble Numbat”}, the codename for the 24.04 LTS release, due to its long-term support and compatibility with the \texttt{ROCm 6.3.4} release.

The use of \texttt{ARG} is employed to explicitly pin the versions of critical dependencies, namely \texttt{ROCm} and \texttt{PyTorch}, via the variables \texttt{ROCM\_VER} and \texttt{TORCH\_VER}. This ensures that all downstream installation steps rely on versions that are known to be mutually compatible. While \texttt{ARG} also enables modular reuse, in this context it primarily serves to enforce version consistency and reproducibility across builds, a best practice in containerized environments dealing with hardware-accelerated libraries.

Subsequently, the \texttt{ENV} instructions define environment variables within the container runtime. Setting \texttt{DEBIAN\_FRONTEND=noninteractive} ensures that APT operations proceed without prompting for user interaction, which is essential in non-interactive Docker builds. The \texttt{PATH=/opt/venv/bin:\$PATH} declaration prepends the virtual environment path, ensuring that any installed Python binaries or tools are prioritized during execution. This is important for maintaining isolation from system-level dependencies.

Finally, the working directory is set to \texttt{/tmp} via \texttt{WORKDIR}, establishing a neutral staging area for all initial setup operations, such as downloading packages and building local wheel caches.

\vspace{1em}

\begin{minipage}{\textwidth}
	\captionsetup{type=listing}
	\caption*{Listing~\ref{lst:dockerfile}~(continued)}

	\begin{minted}[firstnumber=14,
                    breaklines,
                    breakanywhere=true,
                    fontsize=\footnotesize]{docker}
                    # ───────────────────────── 0. basic packages + venv ──────────────────────────
                    RUN apt-get update && \
                        apt-get install -y --no-install-recommends \
                            python3-full python3-venv python3-pip \
                            git wget ca-certificates rsync dialog && \
                        rm -rf /var/lib/apt/lists/*

                    RUN python3 -m venv /opt/venv && \
                        pip install --no-cache-dir --upgrade pip wheel

    \end{minted}
\end{minipage}

This block installs the essential system-level packages required to support Python development and container orchestration. The process begins with \texttt{apt-get update}, which refreshes the package index to ensure the latest metadata is available for installation. This is followed by a carefully scoped \texttt{apt-get install} command that uses the \texttt{--no-install-recommends} flag to suppress installation of non-essential packages, thereby reducing image bloat and limiting the build to explicitly specified dependencies.

The selected packages include:

\begin{itemize}
	\item \texttt{python3-full}, \texttt{python3-venv}, and \texttt{python3-pip}: These provide access to Python's full standard library, support for creating isolated virtual environments, and the package installer \texttt{pip}, respectively. The inclusion of \texttt{python3-full} ensures compatibility with scripts or libraries that may depend on optional modules not present in minimal distributions.
	\item \texttt{git} and \texttt{wget}: Required for cloning external repositories and downloading resources over HTTP or HTTPS. These are essential for fetching the \texttt{BigCode} evaluation harness and prebuilt \texttt{PyTorch} wheels from AMD's repositories.
	\item \texttt{ca-certificates}: Ensures the system trusts root certificate authorities, which is critical for securely accessing external services via HTTPS (e.g., Hugging Face or GitHub).
	\item \texttt{rsync} and \texttt{dialog}: Lightweight utilities that improve scripting flexibility and are occasionally required by package installers.
\end{itemize}

Once the necessary packages are installed, \texttt{rm -rf /var/lib/apt/lists/*} is executed to remove cached package metadata. This further reduces the final image size and prevents stale index data from persisting across layers.

The second \texttt{RUN} command initializes a virtual environment in \texttt{/opt/venv} using \texttt{python3 -m venv}. This isolates all subsequent Python package installations from the system Python interpreter. Immediately after, \texttt{pip} and \texttt{wheel} are upgraded to their latest available versions to ensure compatibility with modern packaging standards. This is especially important when installing packages that use binary wheel formats or require PEP 517/518-based builds.

Together, these instructions establish a minimal, reproducible, and robust foundation for Python-based evaluation workflows within the containerized environment.

\vspace{1em}

\begin{minipage}{\textwidth}
	\captionsetup{type=listing}
	\caption*{Listing~\ref{lst:dockerfile}~(continued)}

	\begin{minted}[firstnumber=24,
                    breaklines,
                    breakanywhere=true,
                    fontsize=\footnotesize]{docker}
                    # ───────────────────────── 1. ROCm meta-package (WSL, no-DKMS) ───────────────
                    WORKDIR /tmp
                    RUN wget -q https://repo.radeon.com/amdgpu-install/6.3.4/ubuntu/noble/amdgpu-install_6.3.60304-1_all.deb && \
                        apt-get update && \
                        apt-get install -y ./amdgpu-install_6.3.60304-1_all.deb && \
                        rm -f amdgpu-install_6.3.60304-1_all.deb && \
                        amdgpu-install -y --usecase=wsl,rocm --no-dkms && \
                        rm -rf /var/lib/apt/lists/*

    \end{minted}
\end{minipage}

This block installs the ROCm base stack required for enabling GPU-accelerated computation on AMD hardware within the container. The working directory is first set to \texttt{/tmp} using \texttt{WORKDIR}, which serves as a temporary staging area for downloading and installing intermediate assets.

The main task here is to install the \texttt{amdgpu-install} meta-package provided by AMD, which orchestrates the configuration of ROCm components. The installation begins by retrieving the Debian package \texttt{amdgpu-install\_6.3.60304-1\_all.deb} via \texttt{wget}. This specific version corresponds to \texttt{ROCm 6.3.4} and is hosted on AMD’s official package repository for Ubuntu \texttt{noble}.

After refreshing the package index with \texttt{apt-get update}, the downloaded package is installed using \texttt{apt-get install}. Immediately afterward, the \texttt{.deb} file is removed via \texttt{rm -f} to conserve disk space and avoid leaving unnecessary installation artifacts in the image.

The command \texttt{amdgpu-install -y --usecase=wsl,rocm --no-dkms} is then executed to install only the components relevant to ROCm and Windows Subsystem for Linux (WSL), while deliberately omitting kernel module installation via the \texttt{--no-dkms} flag. This is critical: since WSL2 lacks full kernel support, attempting to build or load DKMS (Dynamic Kernel Module Support) drivers would fail or cause instability. The use case specification \texttt{wsl,rocm} ensures that only userspace ROCm libraries are installed, those necessary for leveraging GPU compute inside the container without relying on kernel-level drivers.

Finally, the block concludes with a standard cleanup command \texttt{rm -rf /var/lib/apt/lists/*}, which removes cached APT metadata and further reduces the image size, helping maintain a lightweight and reproducible container.

\vspace{1em}

\begin{minipage}{\textwidth}
	\captionsetup{type=listing}
	\caption*{Listing~\ref{lst:dockerfile}~(continued)}

	\begin{minted}[firstnumber=33,
                    breaklines,
                    breakanywhere=true,
                    fontsize=\footnotesize]{docker}
                    # ───────────────────────── 2. ROCm PyTorch wheels ────────────────────────────
                    WORKDIR /tmp/wheels
                    RUN set -e; base=https://repo.radeon.com/rocm/manylinux/rocm-rel-6.3.4; \
                        wget -q ${base}/torch-2.4.0%2Brocm6.3.4.git7cecbf6d-cp312-cp312-linux_x86_64.whl && \
                        wget -q ${base}/torchvision-0.19.0%2Brocm6.3.4.gitfab84886-cp312-cp312-linux_x86_64.whl && \
                        wget -q ${base}/pytorch_triton_rocm-3.0.0%2Brocm6.3.4.git75cc27c2-cp312-cp312-linux_x86_64.whl && \
                        wget -q ${base}/torchaudio-2.4.0%2Brocm6.3.4.git69d40773-cp312-cp312-linux_x86_64.whl && \
                        pip uninstall -y torch torchvision torchaudio pytorch-triton-rocm || true && \
                        pip install --no-cache-dir *.whl && \
                        rm -rf /tmp/wheels

                    # remove conflicting lib bundled with torch
                    RUN python - <<'PY'
                    import torch, pathlib, glob, os
                    libdir = pathlib.Path(torch.__file__).parent / "lib"
                    for so in glob.glob(str(libdir / 'libhsa-runtime64.so*')):
                        print('Removing', so); os.remove(so)
                    PY

    \end{minted}
\end{minipage}

This stage prepares the environment for installing ROCm-compatible \texttt{PyTorch} packages that are officially provided by AMD for use with the \texttt{ROCm 6.3.4} release. The working directory is first set to \texttt{/tmp/wheels}, which serves as a temporary location for downloading the wheel files prior to installation.

The \texttt{RUN} instruction begins with \texttt{set -e}, a shell flag that causes the command sequence to terminate immediately if any sub-command fails. This ensures strict build correctness and avoids partial or corrupted installations.

A base URL is defined as a shell variable (\url{https://repo.radeon.com/rocm/manylinux/rocm-rel-6.3.4}), pointing to AMD’s ROCm-specific Python wheel repository. From this repository, four distinct packages are retrieved using \texttt{wget}:

\begin{itemize}
	\item \texttt{torch-2.4.0+rocm6.3.4} - the core \texttt{PyTorch} library compiled with ROCm backend support.
	\item \texttt{torchvision-0.19.0+rocm6.3.4} - utilities for image transformation and dataset handling.
	\item \texttt{pytorch\_triton\_rocm-3.0.0+rocm6.3.4} - a ROCm-compatible build of \texttt{Triton}, supporting fused GPU kernel execution.
	\item \texttt{torchaudio-2.4.0+rocm6.3.4} - the \texttt{PyTorch} audio library with ROCm backend compatibility.
\end{itemize}

Any previously installed versions of these packages (including possible incompatible system defaults) are removed using \texttt{pip uninstall -y}, and the new ROCm-specific wheels are installed via \texttt{pip install --no-cache-dir *.whl}. Finally, the temporary wheel directory is deleted to keep the container lean.

After installation, a known library conflict is resolved by removing \texttt{libhsa-runtime64.so*} from the \texttt{torch} package’s \texttt{lib} directory. This library is bundled with some builds of \texttt{torch}, but its presence can interfere with the ROCm runtime already installed via the meta-package. A short inline Python script uses the \texttt{pathlib} and \texttt{glob} modules to locate and remove this file:

\begin{quote}
	\texttt{for so in glob.glob(str(libdir / 'libhsa-runtime64.so*')): os.remove(so)}
\end{quote}

This proactive cleanup step ensures that the final container relies on a single, consistent copy of the ROCm runtime, avoiding ambiguous or conflicting shared library references at runtime.

\vspace{1em}

\begin{minipage}{\textwidth}
	\captionsetup{type=listing}
	\caption*{Listing~\ref{lst:dockerfile}~(continued)}

	\begin{minted}[firstnumber=52,
                    breaklines,
                    breakanywhere=true,
                    fontsize=\footnotesize]{docker}
                    # ───────────────────────── 3. project requirements ───────────────────────────
                    WORKDIR /workspace
                    COPY requirements.txt .
                    RUN pip install --no-cache-dir -r requirements.txt

    \end{minted}
\end{minipage}

This block installs project-specific Python dependencies by referencing the \texttt{requirements.txt} file, which is first copied into the container’s working directory \texttt{/workspace}. The file contains only two entries: \texttt{sentencepiece} and \texttt{protobuf}.

The \texttt{sentencepiece} package is a widely used tokenizer library, particularly important for transformer-based language models trained with subword units. The \texttt{protobuf} package provides Python bindings for Protocol Buffers, a language-neutral serialization format commonly used in machine learning pipelines for model configuration and data exchange.

Dependencies are installed using \texttt{pip install --no-cache-dir -r requirements.txt}, which ensures a clean installation without storing wheel files in a local cache. This practice reduces the final container image size and avoids unnecessary clutter from temporary files.

Placing this block after all system-level configuration steps (e.g., ROCm stack and PyTorch installation) ensures a clear separation between infrastructure dependencies and lightweight application-level requirements. This ordering also improves reproducibility and simplifies debugging if Python package-related issues arise.

\vspace{1em}

\begin{minipage}{\textwidth}
	\captionsetup{type=listing}
	\caption*{Listing~\ref{lst:dockerfile}~(continued)}

	\begin{minted}[firstnumber=57,
                    breaklines,
                    breakanywhere=true,
                    fontsize=\footnotesize]{docker}
                    # ───────────────────────── 4. BigCode evaluation harness ─────────────────────
                    # Shallow clone (depth-1) and editable install *without* re-installing common deps
                    RUN git clone --depth 1 https://github.com/bigcode-project/bigcode-evaluation-harness.git \
                            /workspace/bigcode-evaluation-harness && \
                        pip install --no-cache-dir --no-deps -e /workspace/bigcode-evaluation-harness
                   
                    # Copy and enable the entrypoint
                    COPY src/scripts/entrypoint.sh /usr/local/bin/entrypoint.sh
                    RUN chmod +x /usr/local/bin/entrypoint.sh
                   
                    ENTRYPOINT ["/usr/local/bin/entrypoint.sh"]
                   
                    WORKDIR /workspace
                    CMD ["bash"]

    \end{minted}
\end{minipage}

This final block integrates the \texttt{BigCode} evaluation harness into the container and sets up the runtime environment.

First, the repository \texttt{bigcode-evaluation-harness} is cloned using a shallow \texttt{git clone} with the \texttt{--depth 1} flag. This optimization fetches only the latest commit, significantly reducing both the download size and build time. The repository is cloned into \texttt{/workspace/bigcode-evaluation-harness} and installed in editable mode using \texttt{pip install -e}. The \texttt{--no-deps} flag ensures that no dependencies are (re)installed, avoiding conflicts or duplication with packages already installed globally or via \texttt{requirements.txt}. This approach allows the harness to be directly editable within the container, which is particularly useful for development and debugging.

Next, the script \texttt{entrypoint.sh} is copied to \texttt{/usr/local/bin/} and made executable using \texttt{chmod +x}. This script acts as the container's \texttt{ENTRYPOINT}, meaning it will be executed automatically when the container starts.

Finally, the working directory is set to \texttt{/workspace}, and the \texttt{CMD} instruction specifies that the container should launch into a \texttt{bash} shell by default if no other command is provided. This setup allows for interactive use of the container during development, debugging, or manual inspection.

\vspace{1em}

\subsection{Entrypoint}

\begin{minipage}{\textwidth}
	\captionsetup{type=listing}
	\caption{Custom entrypoint script for runtime dependency management and non-interactive authentication}
	\label{lst:entrypoint}

	\begin{minted}[linenos,
        breaklines,
        breakanywhere=true,
        fontsize=\footnotesize]{bash}
        #!/usr/bin/env bash
        set -e

        # If HF_HUB_TOKEN is set, log in non-interactively
        if [ -n "$HF_HUB_TOKEN" ]; then
        echo "Logging into Hugging Face CLI..."
        echo "$HF_HUB_TOKEN" | huggingface-cli login --stdin
        fi

        # 1. Install any missing pip packages
        for pkg in ninja einops transformers packaging setuptools wheel triton; do
        if ! pip show "$pkg" >/dev/null 2>&1; then
            echo "Installing $pkg..."
            pip install "$pkg"
        fi
        done

        # 2. Clone & install causal-conv1d if absent
        if [ ! -d /tmp/causal-conv1d ]; then
        git clone https://github.com/Dao-AILab/causal-conv1d /tmp/causal-conv1d
        pushd /tmp/causal-conv1d
            pip install .
        popd
        fi

        # 3. Clone & install mamba if absent
        if [ ! -d /tmp/mamba ]; then
        git clone https://github.com/state-spaces/mamba /tmp/mamba
        pushd /tmp/mamba
            pip install . --no-build-isolation
        popd
        fi

        # 4. Clean up clones
        rm -rf /tmp/causal-conv1d /tmp/mamba

        exec "$@"
    \end{minted}
\end{minipage}


The custom \texttt{entrypoint.sh} script is executed at container runtime and plays a critical role in completing the environment setup for ROCm-based model evaluation. Rather than bundling all dependencies during the Docker build phase, this script defers the installation of several key Python packages to runtime. This design decision was informed by persistent compatibility issues encountered during build-time when installing packages such as \texttt{transformers}, \texttt{triton}, and particularly \texttt{mamba} in conjunction with ROCm-specific builds of \texttt{PyTorch}.

The script first performs a non-interactive login to Hugging Face if the environment variable \texttt{HF\_HUB\_TOKEN} is set. This enables seamless access to gated models during benchmarking without requiring user input, given that a valid access token is provided.

Next, it ensures that essential Python packages required by some models (e.g., \texttt{Codestral-7B}) are available. It checks whether each package is already installed using \texttt{pip show} and installs any missing ones via \texttt{pip install}. This includes utilities such as \texttt{einops}, \texttt{ninja}, \texttt{transformers}, and \texttt{triton}, which were excluded from the Dockerfile due to build-time conflicts that do not manifest at runtime.

A critical section of the script addresses the installation of two auxiliary repositories: \texttt{causal-conv1d} and \texttt{mamba}. These libraries are required for evaluation but must be built from source. Importantly, \texttt{mamba} fails to build correctly under ROCm when installed with default \texttt{pip} isolation. This is due to its \texttt{pyproject.toml} requesting CUDA-specific builds of \texttt{torch} during isolated setup, which results in misidentification of the available backend and failure of HIP detection logic in its \texttt{setup.py}.

To circumvent this issue, the script installs \texttt{mamba} using \texttt{pip install . --no-build-isolation}, ensuring that the ROCm-specific \texttt{torch} installation already present in the environment is correctly reused. This approach is consistent with the workaround described in a public GitHub issue \autocite{mamba_rocm_conflict}, where the root cause was traced to a broken default environment isolation procedure under ROCm. In that issue, the developers suggest using \texttt{--no-build-isolation} and, optionally, applying AMD's ROCm patch \autocite{rocm_patch_405} to set correct GPU warp sizes.

Once installation is complete, the script cleans up temporary source directories to avoid polluting the container filesystem. It then calls \texttt{exec "\$@"} to forward control to the container's command entrypoint. This ensures that the script behaves transparently while guaranteeing a conflict-free runtime environment.

\vspace{1em}

\clearpage

\subsection{Docker Compose}

\begingroup
\captionsetup{type=listing}
\captionof{listing}{\texttt{docker-compose.yml} service definition
	for ROCm-enabled model benchmarking under WSL2}
\label{lst:docker-compose}

\par\medskip

\begin{minted}[linenos,
                 breaklines,
                 breakanywhere=true,
                 fontsize=\footnotesize]{yaml}
services:
  sandbox:
    container_name: sandbox
    build:
      context: .
      dockerfile: Dockerfile  
    image: sandbox:latest
    env_file: .env
    environment:
      PYTHONUNBUFFERED: "1"
      HF_ALLOW_CODE_EVAL: "1"
      HF_DATASETS_ALLOW_CODE_EXECUTION: "1"
      PYTHONPATH: "/app/src"
      TF_CPP_MIN_LOG_LEVEL: "3"
      TF_FORCE_GPU_ALLOW_GROWTH: "true"
        
      DEEPSEEK_CODER_MODEL_PATH:          "/app/cache/deepseek_coder/snapshots/2a050a4c59d687a85324d32e147517992117ed30"
      CODEGEMMA_MODEL_PATH:               "/app/cache/codegemma/snapshots/078cdc51070553d1636d645c9a238f3b0914459a"
      CODELLAMA_MODEL_PATH:               "/app/cache/codellama/snapshots/b97340ccba60c3ccaf91882ffe470a1bbc32e32c"
      CODESTRAL_MODEL_PATH:               "/app/cache/codestral/snapshots/88085f9cdfa832c3aca8a0315a4520cf7558c947"
      MISTRAL_MODEL_PATH:                 "/app/cache/mistral/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db"
      QWEN_MODEL_PATH:                    "/app/cache/qwen/snapshots/c03e6d358207e414f1eca0bb1891e29f1db0e242"
      STARCODER_MODEL_PATH:               "/app/cache/starcoder/snapshots/bb9afde76d7945da5745592525db122d4d729eb1"

    # ROCm-on-WSL privileges
    cap_add: [SYS_PTRACE]
    security_opt: [seccomp:unconfined]
    privileged: true
    devices:
      - /dev/dxg
      - /dev/dri
    group_add: [video]
    ipc: host
    shm_size: 8g

    volumes:
      - ./src:/app/src:ro
      - ./results:/app/results:rw
      - ./bigcode_cache:/app/bigcode_cache

      # Qwen 2.5-Coder-7B entire model cache (snapshots+blobs)
      - /mnt/c/Users/Elias/.cache/huggingface/hub/models--Qwen--Qwen2.5-Coder-7B-Instruct:/app/cache/qwen:ro
      # Meta CodeLlama 7B Python entire model cache (snapshots+blobs)
      - /mnt/c/Users/Elias/.cache/huggingface/hub/models--meta-llama--CodeLlama-7b-Python-hf:/app/cache/codellama:ro
      # BigCode StarCoder2 7B entire model cache (snapshots+blobs)
      - /mnt/c/Users/Elias/.cache/huggingface/hub/models--bigcode--starcoder2-7b:/app/cache/starcoder:ro
      # Mistral v0.3-7B Instruct entire model cache (snapshots+blobs) 
      - /mnt/c/Users/Elias/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3:/app/cache/mistral:ro
      # Mamba-Codestral-7B-v0.1 entire model cache (snapshots+blobs) 
      - /mnt/c/Users/Elias/.cache/huggingface/hub/models--mistralai--Mamba-Codestral-7B-v0.1:/app/cache/codestral:ro
      # Google Codegemma 7B it entire model cache (snapshots+blobs) 
      - /mnt/c/Users/Elias/.cache/huggingface/hub/models--google--codegemma-7b-it:/app/cache/codegemma:ro
      # Deepseek Coder 7B Instruct v1.5 entire model cache (snapshots+blobs) 
      - /mnt/c/Users/Elias/.cache/huggingface/hub/models--deepseek-ai--deepseek-coder-7b-instruct-v1.5:/app/cache/deepseek_coder:ro

      # ROCm WSL helpers
      - /usr/lib/wsl/lib/libdxcore.so:/usr/lib/libdxcore.so:ro
      - /opt/rocm/lib/libhsa-runtime64.so.1:/opt/rocm/lib/libhsa-runtime64.so.1:ro

    working_dir: /app      
    tty: true

networks:
  default:
    name: sandbox_network
  \end{minted}
\endgroup

The provided \texttt{docker-compose.yaml} configuration defines a single service named \texttt{sandbox}, which encapsulates the full runtime environment for benchmarking code generation models under ROCm on WSL2. The service builds from the previously defined \texttt{Dockerfile} and attaches a variety of privileges, devices, environment variables, and volume mounts tailored to GPU-accelerated, large-model evaluation.

The \texttt{build} directive specifies the local build context (\texttt{.}) and the \texttt{Dockerfile} to use. The image is tagged \texttt{sandbox:latest}, and secrets such as Hugging Face tokens are injected via the \texttt{env\_file} directive referencing a local \texttt{.env} file. To clarify what files and folders are available at build time, the structure of the working directory is shown below:

\begin{forest} pic dir tree
	[/
	[bigcode\_cache/]
	[results/
	[bigcode/
	[humaneval/]
	[humanevalplus/]
	[mbpp/]
	[mbppplus/]
	]
	]
	[src/
	[scripts/
	[entrypoint.sh]
	]
	]
	[venv/]
	[.env]
	[docker-compose.yaml]
	[Dockerfile]
	[requirements.txt]
	]
\end{forest}


The \texttt{src/} directory, mounted read-only to \texttt{/app/src}, contains scripts and configuration files needed for benchmarking, such as the \texttt{entrypoint.sh} and evaluation logic. The \texttt{results/} directory is mounted read-write into \texttt{/app/results}, serving as the destination for metrics and generations produced during evaluation. Outputs are further organized into benchmark-specific folders like \texttt{humaneval}, \texttt{humanevalplus}, \texttt{mbpp}, and \texttt{mbppplus}.

To ensure GPU access under WSL2 with ROCm, several kernel-level privileges and device interfaces must be explicitly granted:
\begin{itemize}
	\item \texttt{cap\_add: SYS\_PTRACE} enables debugging and profiling tools that require ptrace system calls.
	\item \texttt{security\_opt: seccomp:unconfined} disables seccomp filtering to allow ROCm’s dynamic libraries to execute necessary syscalls.
	\item \texttt{privileged: true} elevates container privileges to allow ROCm GPU backend functionality within WSL2.
	\item \texttt{devices} maps GPU access interfaces such as \texttt{/dev/dxg} and \texttt{/dev/dri}.
	\item \texttt{group\_add: video} ensures group-level permissions for GPU access match those of the host user session.
	\item \texttt{ipc: host} enables inter-process communication sharing with the host, which is required for ROCm runtime operations and some PyTorch workloads.
	\item \texttt{shm\_size: 8g} increases the container’s shared memory to \texttt{8g}, helping prevent segmentation faults and out-of-memory errors during large-model inference.
\end{itemize}

Several volumes are mounted into the container:
\begin{itemize}
	\item \texttt{./src} $\rightarrow$ \texttt{/app/src} (read-only): mounts evaluation scripts and supporting code.
	\item \texttt{./results} $\rightarrow$ \texttt{/app/results} (read-write): stores generation and metric JSON files grouped by model and benchmark.
	\item Read-only model caches from Hugging Face are mounted under \texttt{/app/cache}, ensuring deterministic access to model snapshots across Qwen, CodeLlama, StarCoder2, Mistral, and Codestral.
	\item ROCm helper libraries (\texttt{libdxcore.so} and \texttt{libhsa-runtime64.so.1}) are mounted from host paths to maintain runtime compatibility.
	\item \texttt{./bigcode\_cache} provides persistent local caching for BigCode tools like \texttt{bigcode-evaluation-harness}.
\end{itemize}

The \texttt{environment} section defines runtime behavior:
\begin{itemize}
	\item \texttt{PYTHONUNBUFFERED=1} enables unbuffered logging to STDOUT.
	\item \texttt{HF\_ALLOW\_CODE\_EVAL=1} is required by Hugging Face transformers for secure execution of code evaluation.
	\item \texttt{HF\_DATASETS\_ALLOW\_CODE\_EXECUTION=1} permits execution of arbitrary code in dataset loading scripts, which is necessary for certain benchmarks.
	\item \texttt{PYTHONPATH=/app/src} ensures internal modules (e.g., \texttt{tasks.apps.yaml}) are discoverable.
	\item \texttt{TF\_CPP\_MIN\_LOG\_LEVEL=3} suppresses verbose TensorFlow logs, showing only errors.
	\item \texttt{TF\_FORCE\_GPU\_ALLOW\_GROWTH=true} enables dynamic GPU memory allocation in TensorFlow, preventing full preallocation of GPU memory.
	\item Each \texttt{MODEL\_PATH} (e.g., \texttt{QWEN\_MODEL\_PATH}) maps to the relevant snapshot directory inside the mounted cache, which avoids unpredictable re-downloading or version mismatch.
\end{itemize}

The working directory is set to \texttt{/app}, and \texttt{tty: true} enables interactive container sessions. Though a dedicated network \texttt{sandbox\_network} is defined, this single-service configuration does not require inter-container communication.

Altogether, this configuration captures a reproducible and portable ROCm-based development environment, enabling high-throughput model evaluation using cached resources and minimal host-side interference.
