%%=============================================================================
%% Conclusie
%%=============================================================================

\chapter{\IfLanguageName{dutch}{Conclusie}{Discussion}}%
\label{ch:conclusie}

% TODO: Trek een duidelijke conclusie, in de vorm van een antwoord op de
% onderzoeksvra(a)g(en). Wat was jouw bijdrage aan het onderzoeksdomein en
% hoe biedt dit meerwaarde aan het vakgebied/doelgroep? 
% Reflecteer kritisch over het resultaat. In Engelse teksten wordt deze sectie
% ``Discussion'' genoemd. Had je deze uitkomst verwacht? Zijn er zaken die nog
% niet duidelijk zijn?
% Heeft het onderzoek geleid tot nieuwe vragen die uitnodigen tot verder 
%onderzoek?

This thesis set out to evaluate whether self-hosted \glspl{LLM} offer a viable, secure, and performant alternative to proprietary cloud-based systems for code generation. Using a reproducible evaluation pipeline (Chapter~\ref{ch:methodologie}), we benchmarked eight open-weight 7B models locally under consumer-grade GPU constraints and compared their performance with top-tier open-weight and proprietary systems. Functional correctness was assessed using four well-established benchmarks—MBPP, MBPP+, HumanEval, and HumanEval+—selected to represent both practical software tasks and algorithmic reasoning challenges.

\section{Modern self-hosted LLMs can serve as viable, secure alternatives to proprietary cloud-based AI models for code generation}

To address the main research question—\textit{"Can modern self-hosted LLMs serve as viable, secure alternatives to proprietary cloud-based AI models for code generation?"}~(\ref{rq:main})—we synthesized results across all benchmarks and model classes (Chapter~\ref{ch:results}). Based on our findings, the answer is conditionally affirmative. The performance gap between larger open-weight models (13–34 B parameters) and the top proprietary cloud services is surprisingly narrow. On MBPP, several 13–33 B open-weight systems achieve pass@1 scores in the high seventies to low eighties (Figure~\ref{fig:mbpp-openweight}), trailing the best cloud offerings by only 10–15 points (Figure~\ref{fig:mbpp-cloud}). Likewise, on HumanEval, these larger open models score in the 70–75\% range, compared to roughly 90\% for GPT-4 and Gemini Ultra—again a gap of 15–20 points (Figure~\ref{fig:humaneval-cloud}). In practical terms, this means that teams who choose cutting-edge open-weight checkpoints can already deliver code-generation quality that approaches, and almost matches, the cloud frontier.

More notably, our locally hosted 7 B parameter models close much of that remaining distance. Qwen2.5-Coder-7B-Instruct hits 73.8\% on MBPP (Figure~\ref{fig:mbpp-local}), putting it within 5 points of those hefty 13–33 B open-weight peers—and just 20 points behind commercial services. DeepSeek-Coder-7B-Instruct reaches 69.5\% on HumanEval (Figure~\ref{fig:humaneval-local}), outperforming many larger public models. Crucially, both run entirely on a single AMD RX 7900 XT with 20 GB VRAM, using 4-bit quantization and a reproducible Docker+ROCm setup. This demonstrates that organizations of any size—whether a two-person startup or a mid-sized enterprise—can deploy competitive code assistants without investing in massive cloud commitments or specialized hardware.

All models, cloud and local, still exhibit sensitivity to prompt design (see the Δ plots in Figures~\ref{fig:mbpp-delta-local} and~\ref{fig:humaneval-delta-cloud}), underscoring that robustness and consistency must complement raw pass@1 accuracy in any production scenario. Nevertheless, our findings confirm that the luxury of near–state-of-the-art code generation is no longer limited to organizations with large budgets or access to datacenter-scale GPUs.

\section{Research sub-question Review}

\paragraph{SQ1: Which self-hosted LLMs offer the best support for coding tasks while minimizing data leakage risks?}
DeepSeek-Coder-7B-Instruct emerged as the most consistent performer among self-hosted models. It ranked highest on HumanEval (69.5\%) and held strong across HumanEval+ and MBPP+ (Figures~\ref{fig:humanevalplus-local} and~\ref{fig:mbppplus-local}), indicating resilience to prompt variation and aptitude for both algorithmic and real-world tasks. Qwen2.5-Coder-7B-Instruct outperformed all peers on MBPP (73.8\%) but dropped sharply on HumanEval (28.7\%, Figure~\ref{fig:humaneval-local}), revealing a specialization in pattern-based tasks rather than abstract reasoning. Since both models can run entirely offline, they present strong candidates for minimizing data exposure in enterprise or sensitive development contexts.

\paragraph{SQ2: Can self-hosted LLMs reliably assist in secure software development without introducing vulnerabilities?}
When used within a human-in-the-loop setting, our benchmarks indicate that top-performing local models provide functionally correct outputs at a rate sufficient to support development workflows. While some completions contain logic errors or misunderstood instructions (especially in HumanEval+), no catastrophic failures or security-compromising behaviors were observed under test conditions. Local execution under Docker, ROCm, and WSL2 (Section~\ref{sec:pipeline}) allows for air-gapped deployment and deterministic inference, supporting high-integrity use cases with minimized attack surfaces.

\paragraph{SQ3: How does the performance of self-hosted LLMs compare to cloud-based AI tools in coding tasks?}
The strongest proprietary systems—including GPT-4o and o1 Preview—achieve pass@1 scores upwards of 96\% on both MBPP and HumanEval (Figures~\ref{fig:mbpp-cloud} and~\ref{fig:humaneval-cloud}). Nonetheless, the best local 7B models now operate within 15--20 percentage points of these models across benchmarks, with considerably narrower margins in tasks such as MBPP (Figure~\ref{fig:mbppplus-local}). The performance delta is no longer prohibitive, particularly for organizations prioritizing privacy, reproducibility, or infrastructure control. Importantly, even cloud models experienced performance drops of 5--10\% on MBPP+ and HumanEval+ (Figures~\ref{fig:mbpp-delta-cloud} and~\ref{fig:humaneval-delta-cloud}), underscoring that robustness remains an open challenge across the board.

\paragraph{SQ4: What are the deployment challenges and resource requirements of running LLMs locally?}
All local benchmarks were executed on a single AMD RX 7900 XT GPU with 20GB VRAM, using 4-bit quantized weights and containerized orchestration. Inference latency ranged from one to 43 seconds depending on sequence length and prompt complexity. These findings confirm that 7B models are deployable under modest infrastructure constraints. Larger models—such as 13B or 34B open-weight variants—deliver stronger results but remain inaccessible to most local setups due to elevated VRAM and compute demands (Figure~\ref{fig:mbpp-openweight}).

\paragraph{SQ5: What best practices should organizations follow when integrating self-hosted AI coding assistants?}
Based on our evaluation, organizations should select models with strong benchmark performance (e.g., DeepSeek or Qwen2.5), permissive licensing, instruction tuning, and quantization support. Reproducible Docker containers with pinned versions, deterministic inference settings, and thorough logging are essential. Evaluation should extend beyond HumanEval to include MBPP, MBPP+, and task-specific probes (Chapter~\ref{ch:results}) to avoid overfitting to narrow capabilities. Lastly, deployment should always preserve human oversight and auditability, particularly in regulated or safety-critical environments.

\section{Final Reflections}

The rapid advance of open-weight LLMs now makes secure, on-premise code generation a practical reality. While proprietary models still lead in absolute performance, their margin of superiority is shrinking—and not always accompanied by better generalization. In some cases, locally deployed 7B models demonstrate greater prompt stability than their larger, cloud-based counterparts when transitioning from base to augmented benchmarks (Figures~\ref{fig:mbpp-delta-local},~\ref{fig:humaneval-delta-openweight},~\ref{fig:mbpp-delta-cloud}).

Perhaps most notably, the ability to deploy performant models locally no longer presupposes access to large-scale infrastructure. Our findings show that meaningful levels of functional accuracy are attainable using consumer-grade hardware and open-source tooling, effectively lowering the barrier to entry. This shifts the conversation from enterprise feasibility to strategic readiness: even smaller organizations now possess the technical and financial means to operate secure AI systems in-house.

This thesis has demonstrated that self-hosted LLMs, when carefully selected and deployed, can support real-world coding tasks with surprising accuracy, security, and flexibility. The challenge no longer lies in access to tooling—it lies in the precision of implementation. As future work, we recommend research into low-rank adaptation (LoRA) for task-specific fine-tuning, hybrid deployment models, and safety-focused evaluation protocols. What was once a theoretical alternative to cloud-based code generation has now matured into an actionable option.
